CONTENTS {{{

Linear systems and gaussian elimintation     @ 7  `LA-1`
 â”‚ Linear systems and their solutions        @ 7  `LA-1-1`
 â”‚ Elementary row operations                @ 12  `LA-1-2`
 â”‚ Row echelon forms                        @ 14  `LA-1-3`
 â”‚ Gaussian elimintation                    @ 18  `LA-1-4`
 â”” Homogeneous linear systems               @ 29  `LA-1-5`

Matrices                                    @ 40  `LA-2`
 â”‚ Introduction to matrices                 @ 40  `LA-2-1`
 â”‚ Matrix operations                        @ 43  `LA-2-2`
 â”‚ Inverses of square matrices              @ 52  `LA-2-3`
 â”‚ Elementary matrices                      @ 55  `LA-2-4`
 â”” Determinants                             @ 65  `LA-2-5`

Vector spaces                               @ 90  `LA-3`
 â”‚ Euclidean n-spaces                       @ 90  `LA-3-1`
 â”‚ Linear combinations and linear spans     @ 95  `LA-3-2`
 â”‚ Subspaces                               @ 104  `LA-3-3`
 â”‚ Linear independence                     @ 108  `LA-3-4`
 â”‚ Bases                                   @ 112  `LA-3-5`
 â”‚ Dimensions                              @ 117  `LA-3-6`
 â”” Transition matrices                     @ 122  `LA-3-7`

Vector spaces associated with matrices     @ 136  `LA-4`
 â”‚ Row spaces and column spaces            @ 136  `LA-4-1`
 â”‚ Ranks                                   @ 145  `LA-4-2`
 â”” Nullspaces and nullities                @ 147  `LA-4-3`

Orthogonality                              @ 156  `LA-5`
 â”‚ The dot product                         @ 156  `LA-5-1`
 â”‚ Orthogonal and orthonormal bases        @ 159  `LA-5-2`
 â”‚ Best approximations                     @ 166  `LA-5-3`
 â”” Orthogonal matrices                     @ 171  `LA-5-4`

Diagonalization                            @ 184  `LA-6`
 â”‚ Eigenvalues and eigenvectors            @ 184  `LA-6-1`
 â”‚ Diagonalization                         @ 191  `LA-6-2`
 â”‚ Orthogonal diagonalization              @ 198  `LA-6-3`
 â”” Quadratic forms and conic section       @ 202  `LA-6-4`

Linear transformations                     @ 216  `LA-7`
 â”‚ Linear transformations from Râ¿ to Ráµ    @ 216  `LA-7-1`
 â”‚ Ranges and kernels                      @ 221  `LA-7-2`
 â”” Geometric linear transformations        @ 225  `LA-7-3`

}}}

# Linear systems and gaussian elimintation {{{                     @ 7  `LA-1`
Definition 1.0.1 (Standard form) {{{

   x +  y +  z = 1
  2x -  y + 3z = 2
   x + 2y + 7z = 5
  0x - 6y + 2z = 0

}}}
Definition 1.0.2 (Matrix equation form) {{{                             (2.2.17)

ğ—”ğ˜… = ğ—¯, where

ğ—” = [1  1  1;    ğ˜… = [x;    ğ—¯ = [1;
     2 -1  3;         y;         2;
     1  2  7;         z]         5;
     0  6  2]                    0]  

ğ—”: coefficient matrix
ğ˜…: variable matrix
ğ—¯: constant matrix

ğ˜‚ is said to be a solution to the linear system ğ—”ğ˜… = ğ—¯ if ğ—”ğ˜‚ = ğ—¯.

}}}
Definition 1.0.3 (Vector equation form) {{{                             (2.2.18)

xğ—® â‚ + yğ—®â‚‚ + zğ—®â‚ƒ = ğ—¯, where

ğ—®â‚ = [1;    ğ—®â‚‚ = [ 1;    ğ—®â‚ƒ = [1;
      2;          -1;          3;
      1;           2;          7;
      0]           6]          2]

}}}

# Linear systems and their solutions                               @ 7  `LA-1-1`
Remark 1.1.10 {{{

Every system of linear equations has either no solution, only one solution, or
infinitely many solutions.

}}}
Discussion 1.1.11 {{{

Plane representation of 3-equation linear systems.
Best referred to at [./plane-representation.pdf]

}}}

# Elementary row operations                                       @ 12  `LA-1-2`
{{{
They can only be one of:
- Ráµ¢ + aRâ±¼, a âˆˆ R, i â‰  j
- cRáµ¢, c â‰  0
- Ráµ¢ <-> Râ±¼, i â‰  j
}}}
Definition 1.2.6 {{{

Two augmented matrices are said to be row equivalent if one can be ontained from
the other by a series of elementary row operations.

}}}
Definition 1.2.7 {{{

If augmented matrices of two linear systems are row equivalent, then the two
systems share the same set of solutions.

}}}

# Row echelon forms                                               @ 15  `LA-1-3`
Definition 1.3.1 {{{

An augmented matrix is said to be in row-echelon form if:
- all rows consisting of only zeros are at the bottom.
- the leading coefficient (pivot) of a non-zero row is always strictly to the
  right of the leading coefficient of the row above it.

An augmented matrix is said to be in reduced row-echelon form if:
- must already by in row-echelon form.
- leading entry in each non-zero row is a 1 (leading 1)
- each column containing a leading 1 has zeros in all other entries (pivot
  column)

}}}
{{{
row echelon form, but not in reduced row-echelon form:
[1  aâ‚€  aâ‚  aâ‚‚  aâ‚ƒ;
 0   0   2  aâ‚„  aâ‚…;
 0   0   0   1  aâ‚†]

The row echelon form of a matrix is not unique,
while the reduced row-echelon form is unique.
}}}

# Gaussian elimintation                                           @ 18  `LA-1-4`
Algorithm 1.4.2 (Gaussian Elimination) {{{

reduces a matrix to row-echelon form.

}}}
Algorithm 1.4.3 (Gauss-Jordan Elimination) {{{

reduces a matrix to reduced row-echelon form.

}}}
Remark 1.4.8.1 {{{

A linear system has no solution (inconsistent) if:
the last column of a row-echelon form of the augmented matrix is a pivot column,
i.e. there is a row with a non-zero last entry but zero elsewhere.

}}}
Remark 1.4.8.2 {{{

A consistent linear system has only one solution if:
except the last column, every column if a row-echelon form of the augmented
matrix is a pivot column.

In other words, a consistent linear system has exactly one solution if:  
number of variables = number of non-zero rows in REF.

}}}
Remark 1.4.8.3 {{{

A consistent linear system has infinitely many solutions if:
apart from the last column, a row-echelon form of the augmented matrix has at
least one more non-pivot column.

In other words, a consistent linear system has infinitely many solutions if:  
number of variables > number of non-zero rows in row-echelon form.

}}}

# Homogeneous linear systems                                      @ 29  `LA-1-5`
Definition 1.5.1 {{{

A linear system is said to be homogeneous if it has only zeros as constants.

}}}
Remark 1.5.4.1 {{{

A homogeneous linear system has either only the trivial solution or infinitely
many solutions in addition to the trivial solution.

}}}
Remark 1.5.4.2 {{{

A homogeneous linear system with more unknowns than equations has infinitely
many solutions.

}}}
}}}
# Matrices {{{                                                    @ 40  `LA-2`

# Introduction to matrices                                        @ 40  `LA-2-1`
Definition 2.1.1 {{{

 1. Matrix: a rectangular array of numbers.
 2. Entries: numbers in the array.
 3. Size: given by m Ã— n where
    m = # of rows
    n = # of columns.
 4. (i, j)-entry of a matrix is the number at the iáµ—Ê° row and jáµ—Ê° column.

}}}
Definition 2.1.3 {{{

Column matrix/vector is a matrix with only one column.

}}}
Definition 2.1.7 {{{

square matrix: a matrix where row = column.
- diagonal entries: entries where index of row and column are the same.

diagonal matrix: a matrix where all non-diagonal entries = 0.

scalar matrix: diagonal matrix with all diagonal entries the same.

identity matrix: diagonal matrix with all diagonal entries = 1. Denoted by ğ—œ.

zero matrix: all entries = 0. Denoted by ğŸ¬.

symmetric matrix: a matrix which remains the same when transposed.

upper triangular matrix: all entries below diagonal are zero.

lower triangular matrix: all entries above diagonal are zero.

}}}

# Matrix operations                                               @ 43  `LA-2-2`
Definition 2.2.1 {{{

Two matrices are said to be equal if they have the same size and their
corresponding entries are equal.

}}}
Definition 2.2.3 {{{

Let ğ—” = (aáµ¢â±¼)â‚˜â‚“â‚™, ğ—• = (báµ¢â±¼)â‚˜â‚“â‚™ and a scalar c âˆˆ R.
 1. (matrix addition)    ğ—” + ğ—• = (aáµ¢â±¼ + báµ¢â±¼)â‚˜â‚“â‚™
 2. (matrix subtraction) ğ—” + ğ—• = (aáµ¢â±¼ - báµ¢â±¼)â‚˜â‚“â‚™
 3. (scalar multiplication) cğ—” = (caáµ¢â±¼)â‚˜â‚“â‚™

}}}
Theorem 2.2.6 {{{

Let ğ—”, ğ—•, ğ—– be matrices of the same size, and c, d âˆˆ R.
 1. (commutative) ğ—” + ğ—• = ğ—• + ğ—”
 2. (associative) ğ—” + (ğ—• + ğ—–) = (ğ—” + ğ—•) + ğ—–
 3. c(ğ—” + ğ—•) = cğ—” + cğ—•
 4. (c + d)ğ—” = cğ—” + dğ—”
 5. c(dğ—”) = (cd)ğ—” = d(cğ—”)
 6. ğ—” + ğŸ¬ = ğŸ¬ + ğ—” = ğ—”
 7. ğ—” - ğ—” = ğŸ¬
 8. 0ğ—” = ğŸ¬

}}}
Definition 2.2.8 (matrix multiplication) {{{

Let ğ—” = (aáµ¢â±¼)â‚˜â‚“â‚š, ğ—• = (báµ¢â±¼)â‚šâ‚“â‚™.
The product ğ—”ğ—• is defined to be an m Ã— n matrix whole (i, j)-entry is

ğ¨_{k = 1}^{p} (aáµ¢â‚–bâ‚–â±¼)

(dot product of transposed iáµ—Ê° row of left matrix and jáµ—Ê° row of right matrix)

}}}
Remark 2.2.10.1 {{{

We can only multiply two matrices ğ—” and ğ—• (in the manner ğ—”ğ—•) when the number of
columns of ğ—” is equal to the number of rows of ğ—•.

}}}
Remark 2.2.10.2 {{{

Matrix multiplication is not commutative. In general, ğ—”ğ—• and ğ—•ğ—” are two
different matrices.

}}}
Remark 2.2.10.3 {{{

ğ—”ğ—• is referred to as the pre-multiplication of ğ—” to ğ—•.
ğ—•ğ—” is referred to as the post-multiplication of ğ—” to ğ—•.

}}}
Remark 2.2.10.4 {{{

ğ—”ğ—• = ğŸ¬ does not imply ğ—” = ğŸ¬ or ğ—• = ğŸ¬.

ğ—”ğ—• = [0]
one possibility is ğ—” = [1 0] and ğ—• = [0; 1]

}}}
Remark 2.2.11 {{{

 1. (associative)  ğ—”(ğ—•ğ—–) = (ğ—”ğ—•)ğ—–
 2. (distributive) ğ—”(ğ—• + ğ—–) = ğ—”ğ—• + ğ—”ğ—–
                   (ğ—” + ğ—•)ğ—– = ğ—”ğ—– + ğ—•ğ—–

}}}
Definition 2.2.12 (powers of square matrices) {{{

Let ğ—” be a square matrix and n be a non-negative integer.

ğ—”â¿ = ğ—œ          (case: n = 0)
   = ğ—” ğ—” â€¦ ğ—”    (case: n > 0)
     n times

}}}
Remark 2.2.14 {{{

 1. ğ—”áµğ—”â¿ = ğ—”áµâºâ¿
 2. Since matrix multiplication is not commutative, in general,
    (ğ—”ğ—•)â¿ â‰  ğ—”â¿ ğ—•â¿ <=> (ğ—”ğ—•)(ğ—”ğ—•) â€¦ (ğ—”ğ—•) â‰  (ğ—” ğ—” â€¦ ğ—”)(ğ—• ğ—• â€¦ ğ—•)

}}}
Definition 2.2.17 {{{

matrix equation form (refer to 1.0.2)

}}}
Definition 2.2.18 {{{

vector equation form (refer to 1.0.3)

}}}
Definition 2.2.19 {{{

Let ğ—” = (aáµ¢â±¼)â‚˜â‚“â‚™. The transpose of ğ—” (ğ—”áµ€ in writing, ğ—”' in octave) is the
n Ã— m matrix whose (i, j)-entry is aâ±¼áµ¢.

}}}
Theorem 2.2.22 {{{

- (ğ—”áµ€)áµ€ = ğ—”
- (ğ—” + ğ—•)áµ€ = ğ—”áµ€ + ğ—•áµ€
- (cğ—”)áµ€ = c(ğ—”áµ€)
- (ğ—”ğ—•)áµ€ = ğ—•áµ€ğ—”áµ€
-
- }}}

# Inverses of square matrices                                     @ 52  `LA-2-3`
Definition 2.3.2 {{{

Let ğ—” be a square matrix of order n. Then ğ—” is said to be invertible if there
exists a square matrix ğ—• of order n such that ğ—”ğ—• = ğ—œ and ğ—•ğ—” = ğ—œ. ğ—• is called the
inverse of ğ—”. ğ—” matrix is singular if it has no inverse.

}}}
Remark 2.3.4 {{{

 1. (cancellation law) Let ğ—” be an invertible square matrix.
    ğ—”ğ—• = ğ—”ğ—– <=> ğ—• = ğ—–
    ğ—•ğ—” = ğ—–ğ—” <=> ğ—• = ğ—–

}}}
Theorem 2.3.5 (uniqueness of inverse) {{{

If ğ—• and ğ—– are inverses of a square matrix ğ—”, then ğ—• = ğ—–

}}}
Theorem 2.3.9 {{{

Let ğ—”, ğ—• be two invertible matrices of the same size, and c be a
non-zero scalar.

 1. cğ—” is invertible and (cğ—”)â»Â¹ = 1/c (ğ—”â»Â¹)
 2. ğ—”áµ€ is invertible and (ğ—”áµ€)â»Â¹ = (ğ—”â»Â¹)áµ€
    {{{
      ğ—œ = ğ—œáµ€ = (ğ—”â»Â¹ğ—”)áµ€ = ğ—”áµ€(ğ—”â»Â¹)áµ€
      Now since ğ—”áµ€ times (ğ—”â»Â¹)áµ€ gives ğ—œ,
      (ğ—”â»Â¹)áµ€ is the inverse of ğ—”áµ€
      => (ğ—”áµ€)â»Â¹ = (ğ—”â»Â¹)áµ€
    }}}
 3. ğ—”â»Â¹ is invertible and (ğ—”â»Â¹)â»Â¹ = ğ—”
    {{{
      Let (ğ—”â»Â¹)â»Â¹ = ğ—•
      => ğ—• is the inverse of ğ—”â»Â¹,
      => ğ—•ğ—”â»Â¹ = ğ—œ
      By definition, ğ—• = ğ—” satisfies that.
      Since all inverses are unique,
      ğ—• can only take the value of ğ—”.
      => (ğ—”â»Â¹)â»Â¹ = ğ—”
    }}}
 4. ğ—”ğ—• is invertible and (ğ—”ğ—•)â»Â¹ = ğ—•â»Â¹ğ—”â»Â¹
    {{{
      firstly, (ğ—”ğ—•)â»Â¹ â‰  ğ—”â»Â¹ğ—•â»Â¹
      ğ—”ğ—•ğ—•â»Â¹ğ—”â»Â¹ = ğ—”(ğ—•ğ—•â»Â¹)ğ—”â»Â¹
               = ğ—”ğ—œğ—”â»Â¹
               = ğ—œ
      => (ğ—”ğ—•)â»Â¹ = ğ—•â»Â¹ğ—”â»Â¹
    }}}

}}}
Remark 2.3.10 {{{

By Theorem 2.3.9.4, if ğ—”â‚, ğ—”â‚‚, â€¦, ğ—”â‚– are invertible matrices of the same
size, then (ğ—”â‚ ğ—”â‚‚ â€¦ ğ—”â‚–) is invertible
(ğ—”â‚ğ—”â‚‚ â€¦ ğ—”â‚–)â»Â¹ = ğ—”â‚–â»Â¹ â€¦ ğ—”â‚‚â»Â¹ ğ—”â‚â»Â¹

}}}
Examples {{{
Q:
Suppose ğ—” is an invertible matrix of order n. Then for any ğ—¯ âˆˆ Râ¿, 
 1. ğ—”ğ˜… = ğ—¯ is consistent, and
 2. the solution to ğ—”ğ˜… = ğ—¯ is unique.

  1.
  Let ğ˜‚ = ğ—”â»Â¹ğ—¯
  Then ğ—”ğ˜‚ = ğ—”ğ—”â»Â¹ğ—¯ = ğ—¯
  so then ğ˜‚ is a solution. (substitutable into ğ˜… in ğ—”ğ˜… = ğ—¯)
  => 1. is true. (ğ—”ğ˜… = ğ—¯ is consistent)

  2.
  Suppose ğ˜ƒ is a solution
  Then ğ—”ğ˜ƒ = ğ—¯.
  Applying ğ—”â»Â¹,
  ğ—”â»Â¹ğ—”ğ˜ƒ = ğ—”â»Â¹ğ—¯
  ğ˜ƒ = ğ˜‚
  so if we can find a solution ğ˜ƒ, it will always be the same as solution ğ˜‚.
  => 2. is true. (solution to ğ—”ğ˜… = ğ—¯ is unique)

Q:
Suppose ğ—” is an invertible matrix of order n. Then ğ—”ğ˜… = ğŸ¬ has only the
trivial solution.

  ğ—”ğ˜… = ğŸ¬
  => ğ—”â»Â¹ğ—”ğ˜… = ğ—”â»Â¹ğŸ¬
  => ğ˜… = ğŸ¬
  since the solution is shown to be unique,
  => statement is true (ğ—”ğ˜… = ğŸ¬ has only the trivial solution)

Q:
Suppose ğ—” and ğ—• are two square matrices of order n. ğ—”ğ—• is invertible
if and only if ğ—” and ğ—• are invertible. True or false?

  (<==) ğ—”ğ—•â»Â¹ = ğ—•â»Â¹ğ—”â»Â¹
    if ğ—” and ğ—• are invertible, then
    ğ—”ğ—• ğ—•â»Â¹ğ—”â»Â¹ = ğ—œ
    => ğ—•â»Â¹ğ—”â»Â¹ is the unique inverse of ğ—”ğ—•
    => ğ—”ğ—• is invertible

  (==>) âˆƒğ—”ğ—•â»Â¹ => âˆƒğ—– s.t. ğ—–(ğ—”ğ—•) = ğ—œ = ğ—”ğ—•(ğ—–)
    ğ—œ = ğ—–(ğ—”ğ—•) = (ğ—–ğ—”)ğ—• => ğ—–ğ—” is the unique inverse of ğ—• => ğ—• is invertible
    ğ—œ = (ğ—”ğ—•)ğ—– = ğ—”(ğ—•ğ—–) => ğ—•ğ—– is the unique inverse of ğ—” => ğ—” is invertible

Q:
Let ğ—” be a square matrix with ğ—”Â² + ğ—” = ğ—œ. Show that ğ—”â»Â¹ = ğ—” + ğ—œ.

  Let ğ—• be the inverse of ğ—”.
  ğ—”(ğ—” + ğ—œ) = ğ—”Â² + ğ—”
           = ğ—œ
  So ğ—” + ğ—œ is the right inverse of ğ—”, which is the unique inverse of ğ—”.

Q:
Suppose that ğ—” and ğ—• are invertible matrices of the same size and ğ—” + ğ—• is
invertible. Show that
 1. ğ—”â»Â¹ + ğ—•â»Â¹ is invertible, and
 2. (ğ—” + ğ—•)â»Â¹ = ğ—”â»Â¹(ğ—”â»Â¹ + ğ—•â»Â¹)â»Â¹ğ—•â»Â¹

  1.
  ğ—•â»Â¹(ğ—” + ğ—•)ğ—”â»Â¹ = (ğ—•â»Â¹ğ—” + ğ—œ)ğ—”â»Â¹
                = ğ—”â»Â¹ + ğ—•â»Â¹
  (ğ—”â»Â¹ + ğ—•â»Â¹)â»Â¹ = ğ—”(ğ—” + ğ—•)â»Â¹ğ—•
  => ğ—”â»Â¹ + ğ—•â»Â¹ is invertible

  2.
  From the last section,
     (ğ—”â»Â¹ + ğ—•â»Â¹)â»Â¹    = ğ—”(ğ—” + ğ—•)â»Â¹ğ—•
  ğ—”â»Â¹(ğ—”â»Â¹ + ğ—•â»Â¹)â»Â¹    =  (ğ—” + ğ—•)â»Â¹ğ—•
  ğ—”â»Â¹(ğ—”â»Â¹ + ğ—•â»Â¹)â»Â¹ğ—•â»Â¹ =  (ğ—” + ğ—•)â»Â¹

}}}

# Elementary matrices                                             @ 55  `LA-2-4`
Discussion 2.4.2 {{{

All elementary row operations can be re-written as pre-multiplying the matrix
operated on with an elementary matrix.

}}}
Definition 2.4.3 {{{

A square matrix is called an elementary matrix if it can be obtained from an
identity matrix by doing a single elementary row operation.

}}}
Remark 2.4.4 {{{

All elementary matrices are invertible and their inverses are also elementary
matrices.

}}}
Theorem 2.4.7 {{{

If ğ—” is a square matrix, then the following statements are equivalent:

 1. ğ—” is invertible.
 2. ğ—”ğ˜… = ğŸ¬ has only the trivial solution.
 3. rref(ğ—”) = ğ—œ.
 4. ğ—” can be expressed as a product of elementary matrices.
    {{{
      1 => 2:
      If ğ—” is invertible, then ğ—”ğ˜… = ğŸ¬ implies
      ğ˜… = ğ—œğ˜… = ğ—”â»Â¹ğ—”ğ˜… = ğ—”â»Â¹ğŸ¬ = ğŸ¬
      and hence the system has only the trivial solution ğ˜… = ğŸ¬.
    
      2 => 3:
      Suppose ğ—”ğ˜… = ğŸ¬ has only the trivial solution. Since the number of columns in ğ—”
      is equal to the number of rows in ğ—”, the reduced row-echelon form of the
      augmented matrix (ğ—” | ğŸ¬) of the system ğ—”ğ˜… = ğŸ¬ cannot have any zero rows
      (1.4.8.2). Hence the reduced row-echelon form of (ğ—” | ğŸ¬) is (ğ—œ | ğŸ¬). Hence
      rref(ğ—”) = ğ—œ.
    
      3 => 4:
      Since the reduced row-echelon form of ğ—” is ğ—œ, there exist elementary matrices
      ğ—˜â‚, ğ—˜â‚‚, â€¦ ğ—˜â‚– such that
      ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚ ğ—” = ğ—œ
      and hence
      ğ—” = ğ—˜â‚â»Â¹ ğ—˜â‚‚â»Â¹ â€¦ ğ—˜â‚–â»Â¹ ğ—œ
        = ğ—˜â‚â»Â¹ ğ—˜â‚‚â»Â¹ â€¦ ğ—˜â‚–â»Â¹
      where ğ—˜â‚â»Â¹, ğ—˜â‚‚â»Â¹, â€¦, ğ—˜â‚–â»Â¹ are also elementary matrices.
    
      4 => 1:
      Suppose ğ—” is a product of elementary matrices. Since all elementary matrices
      are invertible, ğ—” is invertible (2.3.10)
    }}}

}}}
Discussion 2.4.8 {{{

Let ğ—” be an invertible matrix of order n and let ğ—˜â‚, ğ—˜â‚‚, â€¦, ğ—˜â‚– be
elementary matrices such that
ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚ ğ—” = ğ—œ.
Post multiply ğ—”â»Â¹ to both sides of the equation:
ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚ ğ—” ğ—”â»Â¹ = ğ—”â»Â¹
=> ğ—”â»Â¹ = ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚
Consider the n Ã— 2n matrix (ğ—” | ğ—œ). We have
ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚ (ğ—” | ğ—œ) = ğ—˜â‚– â€¦ ğ—˜â‚‚ (ğ—˜â‚ ğ—” | ğ—˜â‚ ğ—œ)
                          â€¦
                        = (ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚ ğ—” | ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚ ğ—œ)
                        = (ğ—œ | ğ—”â»Â¹)
This provides us a method to find the inverse of ğ—”.

}}}
Remark 2.4.10 {{{

From 2.4.7, we know that a square matrix is invertible if and only if its
reduced row-echelon form is an identity matrix. Thsi can be used to check
whether a square matrix is invertible. We actually only need to reduce the
matrix to a row-echelon form. If the row-echelon form of a square matrix has no
zero row, then it is invertible (rref will definitely be ğ—œ).

}}}
Theorem 2.4.12 {{{

If ğ—”ğ—• = ğ—œ, then ğ—” and ğ—• are both invertible, and:

 1. ğ—”â»Â¹ = ğ—•
 2. ğ—•â»Â¹ = ğ—”
 3. ğ—•ğ—” = ğ—œ

}}}
Theorem 2.4.14 {{{

Let ğ—” and ğ—• be two matrices of the same order. If ğ—” is singular, then ğ—”ğ—• and ğ—•ğ—”
are singular.

}}}
Example 2.4.13 {{{

Q:
Let ğ—” be a square matrix such that
ğ—”Â² - 3ğ—” - 6ğ—œ = ğŸ¬
Show that ğ—” is invertible.

  Note that
  ğ—”(ğ—” - 3ğ—œ) = ğ—”Â² - 3ğ—”ğ—œ = ğ—”Â² - 3ğ—” = 6ğ—œ

  So ğ—”[1/6 (ğ—” - 3ğ—œ)] = ğ—œ and therefore ğ—” is invertible.

}}}

# Determinants                                                    @ 65  `LA-2-5`
Discussion 2.5.2 (cofactor) {{{

Let ğ—” = (aáµ¢â±¼) be an n Ã— n matrix. Let Máµ¢â±¼ be an (n - 1) Ã— (n - 1) matrix
obtained from ğ—” by deleting the iáµ—Ê° row and jáµ—Ê° column. Then the determinant of
ğ—” is defined as

det(ğ—”) = aâ‚â‚                             if (n == 1)
       = aâ‚â‚ğ—”â‚â‚ + aâ‚â‚‚ğ—”â‚â‚‚ + â€¦ + aâ‚â‚™ğ—”â‚â‚™    if (n >= 1)

where ğ—”áµ¢â±¼ = (-1)â±âºÊ² det(Máµ¢â±¼)
The number ğ—”áµ¢â±¼ is called the (i, j)-cofactor of ğ—”.
This way of defining "determinant" is known as the cofactor expansion.

}}}
Theorem 2.5.6 (cofactor expansions) {{{

For an n Ã— n matrix ğ—” = (aáµ¢â±¼), det(ğ—”) can be expressed as a cofactor expansion
using any row or column of ğ—”.

}}}
Theorem 2.5.8 {{{

If ğ—” is a triangular matrix, then the determinant of ğ—” is equal to the product
of the diagonal entries of ğ—”.

}}}
Theorem 2.5.10 {{{

If ğ—” is a square matrix, then det(ğ—”) = det(ğ—”áµ€).

}}}
Theorem 2.5.12 {{{

 1. The determinant of a square matrix with two identical rows is zero.
 2. The determinant of a square matrix with two identical columns is zero.

}}}
Theorem 2.5.15 {{{

 1. If ğ—• is a square matrix obtained from ğ—” by multiplying one row of ğ—” by a
    constant k
    => det(ğ—•) = k det(ğ—”)
 2. If ğ—• is a square matrix obtained from ğ—” by interchanging two rows of ğ—”
    => det(ğ—•) = -det(ğ—”)
 3. If ğ—• is a square matrix obtained from ğ—” by adding a multiple of one row of ğ—”
    to another row
    => det(ğ—•) = det(ğ—”)
 4. Let ğ—˜ be an elementary matrix of the same size as ğ—”
    => det(ğ—˜ğ—”) = det(ğ—˜)det(ğ—”)

}}}
Remark 2.5.16 {{{

By Theorem 2.5.15, we can use elementary row operations to transform a square
matrix to a triangular matrix and then by Theorem 2.5.8, compute the determinant
accordingly.

}}}
Theorem 2.5.19 {{{

A square matrix ğ—” is invertible if and only if det(ğ—”) â‰  0.
{{{
  Let ğ—” be a square matrix and ğ—˜â‚, ğ—˜â‚‚, â€¦ ğ—˜â‚– be elementary matrices such
  that ğ—• = ğ—˜â‚– â€¦ ğ—˜â‚‚ ğ—˜â‚ ğ—” is the reduced row-echelon form of ğ—”.

  by Theorem 2.5.15.4, det(ğ—•) = det(ğ—˜â‚–) â€¦ det(ğ—˜â‚‚) det(ğ—˜â‚) det(ğ—”)

  (==>) If ğ—” is invertible, then by Theorem 2.4.7, ğ—• = ğ—œ. Hence det(ğ—”) â‰  0.

  (<==, contrapositive) Suppose ğ—” is singular. Then ğ—• has a row consisting
  entirely of zeros. So det(ğ—•) = 0. Since det(ğ—˜áµ¢) â‰  0 for all i, det(ğ—”) must be
  zero.
}}}

}}}
Theorem 2.5.22 {{{

Let ğ—” and ğ—• be two square matrices of order n and c a scalar. Then
 1. det(cğ—”) = câ¿ det(ğ—”)
 2. det(ğ—”ğ—•) = det(ğ—”) det(ğ—•)
 3. if ğ—” is invertible, det(ğ—”â»Â¹) = 1/det(ğ—”)
    {{{
      Suppose ğ—” is invertible. Since ğ—”ğ—”â»Â¹ = ğ—œ,
      det(ğ—”) det(ğ—”â»Â¹) = det(ğ—”ğ—”â»Â¹) = det(ğ—œ) = 1
      clearly.
    }}}

}}}
Theorem 2.5.24 {{{

Let ğ—” be a square matrix of order n. Then the (classical) adjoint of ğ—” is the
matrix n Ã— n matrix.

adj(ğ—”)  =  [ğ—”â‚â‚ ğ—”â‚â‚‚ â€¦ ğ—”â‚â‚™;  =  [ğ—”â‚â‚ ğ—”â‚‚â‚ â€¦ ğ—”â‚™â‚;
            ğ—”â‚‚â‚ ğ—”â‚‚â‚‚ â€¦ ğ—”â‚‚â‚™;      ğ—”â‚â‚‚ ğ—”â‚‚â‚‚ â€¦ ğ—”â‚™â‚‚;
             â‹®      â‹±  â‹® ;       â‹®      â‹±  â‹® ;
            ğ—”â‚™â‚ ğ—”â‚™â‚‚ â€¦ ğ—”â‚™â‚™]áµ€     ğ—”â‚â‚™ ğ—”â‚‚â‚™ â€¦ ğ—”â‚™â‚™]

where ğ—”áµ¢â±¼ is the (i, j)-cofactor of ğ—”.

}}}
Theorem 2.5.25 {{{

Let ğ—” be a square matrix. If ğ—” is invertible, then
ğ—”â»Â¹ = 1/det(ğ—”) Â· adj(ğ—”)

Note that a similar form is

ğ—” Â· adj(ğ—”) = det(ğ—”)ğ—œ

where ğ—” is not required to be invertible.

}}}
{{{
  Let ğ—” = (aáµ¢â±¼) be an n Ã— n matrix and let ğ—”[adj(ğ—”)] = (báµ¢â±¼). Then
  báµ¢â±¼ = aáµ¢â‚ ğ—”â±¼â‚ + aáµ¢â‚‚ ğ—”â±¼â‚‚ + â€¦ + aáµ¢â‚™ ğ—”â±¼â‚™

  By Theorem 2.5.6, báµ¢i = det(ğ—”).

  Note that if i â‰  j, then báµ¢â±¼ = det(M) where M is a matrix with two identical
  rows, and by Theorem 2.5.12, det(M) = 0 = báµ¢â±¼. So then

  ğ—”[adj(ğ—”)] = det(ğ—”) ğ—œ => ğ—”[1/det(ğ—”) adj(ğ—”)] = ğ—œ

  By Theorem 2.4.12, ğ—”â»Â¹ = 1/det(ğ—”) adj(ğ—”)
}}}
Theorem 2.5.27 (Cramer's Rule) {{{

Suppose ğ—”ğ˜… = ğ—¯ is a linear system where ğ—” is an n Ã— n matrix. Let ğ—”áµ¢ be the
matrix obtained from ğ—” by replacing the iáµ—Ê° column of ğ—” by ğ—¯. If ğ—” is
invertible, then the system has only one solution ğ˜…, where
ğ˜… = 1/det(ğ—”) Â· [det(ğ—”â‚);
                det(ğ—”â‚‚);
                   â‹®   ;
                det(ğ—”â‚™)]

}}}
Example 2.5.28 {{{

Let ğ˜… = [xâ‚; and ğ—¯ = [bâ‚;
         xâ‚‚;          bâ‚‚;
         â‹® ;          â‹® ;
         xâ‚™]          bâ‚™]
Since
ğ—”ğ˜… = ğ—¯ <=> ğ˜… = ğ—”â»Â¹ğ—¯ = 1/det(ğ—”) [adj(ğ—”)] ğ—¯
we have
xáµ¢ = 1/det(ğ—”) Â· (bâ‚ğ—”â‚áµ¢ + bâ‚‚ğ—”â‚‚áµ¢ + â€¦ + bâ‚™ğ—”â‚™áµ¢)
   = 1/det(ğ—”) Â· det(ğ—”áµ¢)
for i = 1, 2, â€¦, n

}}}
}}}
# Vector spaces {{{                                               @ 90  `LA-3`

# Euclidean n-spaces                                              @ 90  `LA-3-1`
Discussion 3.1.3 {{{

An n-vector or ordered n-tuple of real numbers has the form

(uâ‚, uâ‚‚, â€¦, uáµ¢ , â€¦, uâ‚™)

where uâ‚, uâ‚‚, â€¦, uâ‚™ are real numbers. The number uáµ¢ in the iáµ—Ê° position
of an n-vector is called the iáµ—Ê° component or the iáµ—Ê° coordinate of the
n-vector.

}}}
Definition 3.1.7 {{{

The set of all n-vectors of real numbers is called the Euclidean n-space or
simply n-space. We use R to denote the set of all real numbers and Râ¿ to denote
the Euclidean n-space.

"ğ˜‚ is a vector in Râ¿" <=> "ğ˜‚ is an n-vector"

i.e. ğ˜‚ âˆˆ Râ¿ <=> ğ˜‚ = (uâ‚, uâ‚‚, â€¦, uâ‚™) for some uâ‚, uâ‚‚, â€¦, uâ‚™ âˆˆ R.

}}}
Notation 3.1.9 {{{

Let S be a finite set. |S| denotes the number of elements contained in S.

}}}
Example 3.1.10 {{{

Let ğ—” = {1, 2, 3, 4}, ğ—• = {(1, 2, 3, 4)}, ğ—– = {(1, 2, 3), (2, 3, 4)}.

Then |ğ—”| = 4, |ğ—•| = 1, |ğ—–| = 2.

}}}

# Linear combinations and linear spans                            @ 95  `LA-3-2`
Definition 3.2.1 (linear combination) {{{

Let ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– be vectors in Râ¿. For any real numbers
câ‚, câ‚‚ , â€¦, câ‚–, the vector

câ‚ğ˜‚â‚ + câ‚‚ğ˜‚â‚‚ + â€¦ + câ‚–ğ˜‚â‚–

is called a linear combination of ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–.

}}}
Example 3.2.2 (summary) {{{

To determine whether a vector ğ˜ƒ is a linear combination of vectors
ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–, we form the linear system

ğ˜ƒ = câ‚ğ˜‚â‚ + câ‚‚ğ˜‚â‚‚ + â€¦ + câ‚–ğ˜‚â‚–

and check if it has a solution (is consistent).
One way is to use Gaussian elimintation and check for pivot rows in last column.

}}}
Definition 3.2.3 (span) {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a set of vectors in Râ¿. Then the set of all
linear combinations of ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–,

{ câ‚ğ˜‚â‚ + câ‚‚ğ˜‚â‚‚ + â€¦ + câ‚–ğ˜‚â‚– | câ‚, câ‚‚, â€¦, câ‚– âˆˆ R },

is called the linear span of S (or the linear span of ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–) and
is denoted by span(S), or span{ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–}

}}}
Example 3.2.4 {{{

Q:
Show that span{(1, 1, 1), (1, 2, 0), (2, 1, 3), (2, 3, 1)} â‰  RÂ³.

  For any vector (x, y, z) âˆˆ RÂ³, we solve the vector equation

  (x, y, z) = a(1, 1, 1) + b(1, 2, 0) + c(2, 1, 3) + d(2, 3, 1)

  where a, b, c, d are variables. The linear system is

    a +  b + 2c + 2d = x
    a + 2b +  c + 3d = y
    a      + 3c +  d = z

  let its augmented matrix be M.

           1  1  2  2  |  x
  ref(M) = 0  1 -1  1  |  y - x
           0  0  0  0  |  y + z - 2x

  The system is inconsistent if y + z - 2x â‰  0.
  For example, if (x, y, z) = (1, 0, 0) where y + z - 2x = -2 â‰  0,
  then the system doesn't have a solution,
  => (1, 0, 0) âˆ‰ span{(1, 1, 1), (1, 2, 0), (2, 1, 3), (2, 3, 1)}.
  Hence span{(1, 1, 1), (1, 2, 0), (2, 1, 3), (2, 3, 1)} â‰  RÂ³.

}}}
Discussion 3.2.5 (determine whether a set of vectors spans Râ¿) {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} âŠ† Râ¿ where ğ˜‚áµ¢ = (aáµ¢â‚, aáµ¢â‚‚, â€¦, aáµ¢â‚™), for
i = {1, 2, â€¦, k}. For any ğ˜ƒ = (vâ‚, vâ‚‚, â€¦, vâ‚™) âˆˆ Râ¿, ğ˜ƒ is contained in
span(S) if and only if the vector equation

ğ˜ƒ = câ‚ ğ˜‚â‚ + câ‚‚ ğ˜‚â‚‚ + â€¦ + câ‚– ğ˜‚â‚–

has a solution for câ‚, câ‚‚, â€¦, câ‚–, i.e. the linear system

  aâ‚â‚câ‚ + aâ‚‚â‚câ‚‚ + â€¦ + aâ‚–â‚câ‚– = vâ‚
  aâ‚â‚‚câ‚ + aâ‚‚â‚‚câ‚‚ + â€¦ + aâ‚–â‚‚câ‚– = vâ‚‚
                      â‹®
  aâ‚â‚™câ‚ + aâ‚‚â‚™câ‚‚ + â€¦ + aâ‚–â‚™câ‚– = vâ‚™

is consistent.

Let ğ—” be the coefficient matrix (1.0.2) of the linear system.
 1. If a row-echelon form of ğ—” does not have any zero row, then the linear system
    is always consistent regardless of the values of vâ‚, vâ‚‚, â€¦, vâ‚™. And
    hence span(S) = Râ¿.
 2. If a row-echelon form of ğ—” has at least one zero row, then the linear system
    is not always consistent and hence span(S) â‰  Râ¿.

}}}
Theorem 3.2.7 {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a set of vectors in Râ¿. If k < n, then S
cannot span Râ¿.

}}}
Example 3.2.8 {{{

 1. One vector cannot span RÂ².
 2. One vector or two vectors cannot span RÂ³.

}}}
Theorem 3.2.9 {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} âŠ† Râ¿.

 1. ğŸ¬ âˆˆ span(S) where ğŸ¬ is the zero vector.
 2. For any ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£ âˆˆ span(S) and câ‚, câ‚‚, â€¦, cáµ£ âˆˆ R,

      câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£ âˆˆ span(S)

}}}
Theorem 3.2.10 (space as subset, subspace) {{{

Let U = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–}
and V = {ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚˜} be subsets of Râ¿.

Then span(U) âŠ† span(V) if and only if
each ğ˜‚áµ¢ is a linear combination of ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚˜.

}}}
Example 3.2.11.2 {{{

Q:
Let ğ˜‚â‚ = (1, 0, 0, 1), ğ˜‚â‚‚ = (0, 1, -1, 2), ğ˜‚â‚ƒ = (2, 1, -1, 4) and
ğ˜ƒâ‚ = (1, 1, 1, 1), ğ˜ƒâ‚‚ = (-1, 1, -1, 1), ğ˜ƒâ‚ƒ = (-1, 1, 1, -1). Show that
span{ğ˜‚â‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ} âŠ† span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ}
but
span{ğ˜‚â‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ} â‰  span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ}

  To show that each ğ˜‚áµ¢, i = 1, 2, 3 is a linear combination of ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ, we
  solve the three linear systems together.
  linearSystems = {
    ağ˜ƒâ‚ + bğ˜ƒâ‚‚ + cğ˜ƒâ‚ƒ = ğ˜‚â‚;
    ağ˜ƒâ‚ + bğ˜ƒâ‚‚ + cğ˜ƒâ‚ƒ = ğ˜‚â‚‚;
    ağ˜ƒâ‚ + bğ˜ƒâ‚‚ + cğ˜ƒâ‚ƒ = ğ˜‚â‚ƒ;
  }
  Written as an augmented matrix, we can convert it to row-echelon form:
  1 -1 -1 | 1 |  0 |  2       1 -1 -1 |  1 |  0 |  2
  1  1  1 | 0 |  1 |  1       0  2  2 | -1 |  1 | -1
  1 -1  1 | 0 | -1 | -1  -->  0  0  2 | -1 | -1 | -3
  1  1 -1 | 1 |  2 |  4       0  0  0 |  0 |  0 |  0
  Since all three systems are consistent, all ğ˜‚áµ¢ are linear combinations of
  ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ. So
    span{uâ‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ} âŠ† span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ}
  On the other hand,
  1  0  2 | 1 | -1 | -1       1  0  2 | 1 | -1 | -1
  0  1  1 | 1 |  1 |  1       0  1  1 | 1 |  1 |  1
  0 -1 -1 | 1 | -1 |  1  -->  0  0  0 | 2 |  0 |  2
  1  2  4 | 1 |  1 | -1       0  0  0 | 0 |  0 |  0
  Since not all three systems are consistent, some ğ˜ƒáµ¢ are not linear combinations
  of ğ˜‚â‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ. So span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ} is not a subset of span{ğ˜‚â‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ},
  Hence
    span{ğ˜‚â‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ} â‰  span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ}.
  (specifically, ğ˜ƒâ‚ and ğ˜ƒâ‚ƒ are not linear combinations of ğ˜‚â‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ.)

}}}
Theorem 3.2.12 {{{

Let ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– be vectors in Râ¿. If ğ˜‚â‚– is a linear combination of
ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–â‚‹â‚, then
  span{ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–â‚‹â‚} = span{ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–â‚‹â‚, ğ˜‚â‚–}

}}}
Discussion 3.2.15 (summary) {{{

 1. Take ğ˜…, ğ˜‚ âˆˆ Râ¿ where ğ˜‚ is a non-zero vector. The set
    L = { ğ˜… + ğ˜„ | ğ˜„ âˆˆ span{ğ˜‚} } is called a line in Râ¿.

 2. Take ğ˜…, ğ˜‚, ğ˜ƒ âˆˆ Râ¿ where ğ˜‚, ğ˜ƒ are non-zero vectors and ğ˜‚ is not a scalar
    multiple of ğ˜ƒ. The set P = { ğ˜… + ğ˜„ | ğ˜„ âˆˆ span{ğ˜‚, ğ˜ƒ} } is called a plane or
    2-plane in Râ¿.

 3. Take ğ˜…, ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚áµ£ âˆˆ Râ¿. The set
    Q = { ğ˜… + ğ˜„ | ğ˜„ âˆˆ span{ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚áµ£ } } is called a k-plane in Râ¿
    where k is the "dimension" of span{ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚áµ£}. (more in `LA-3-6`)

}}}
Proofs {{{
Proof 3.2.7
Follow the notation in Discussion 3.2.5. Since k < n, a row-echelon form of the
coefficient matrix must have at least one zero row. Thus span(S) â‰  Râ¿.

Proof 3.2.9.1
Since ğŸ¬ = 0 ğ˜‚â‚ + 0 ğ˜‚â‚‚ + â€¦ + 0 ğ˜‚â‚–, then ğŸ¬ âˆˆ span(S).

Proof 3.2.9.2
Since ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£ âˆˆ span(S), each ğ˜ƒáµ¢ is a linear combination of
ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–, for i = {1, 2, â€¦, r}. Thus

câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£

is also a linear combination of ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–.
=> câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£ âˆˆ span(S).

Proof 3.2.10
(==>) {
Suppose span(U) âŠ† span(V). Since U âŠ† span(U),
=> U âŠ† span(V)
=> each ğ˜‚áµ¢ in U is a linear combination of ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚˜.
}
(<==) {
Suppose each ğ˜‚áµ¢ is a linear combination of ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚˜. It means that
ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– âˆˆ span(V).
Let ğ˜„ be any vector in span(U).
Then by Theorem 3.2.9.2, ğ˜„ is also a vector in span(V).
=> span(U) âŠ† span(V)
}
}}}

# Subspaces                                                      @ 104  `LA-3-3`
Discussion 3.3.2 (subspace, span) {{{

Let V be a subset of Râ¿. Then V is called a subspace of Râ¿ if V = span(S)
where S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} for some vectors ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– âˆˆ Râ¿. More
precisely, V is called the subspace spanned by S (or the subspace spanned by
ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–). We also say that S spans (or ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– span) the
subspace V.

}}}
Remark 3.3.3 {{{

 1. Let ğŸ¬ be the zero vector in Râ¿. Then the set {ğŸ¬} = span{ğŸ¬} is a subspace of
    Râ¿ and is known as the zero space.

 2. Let ğ—²â‚ = [1; 0; â€¦; 0]; ğ—²â‚‚ = [0; 1; â€¦; 0]; â€¦, ğ—²â‚™ = [0; 0; â€¦; 1] be vectors
    of Râ¿. Any vector ğ˜‚ = (uâ‚, uâ‚‚, â€¦, uâ‚–) âˆˆ Râ¿ can be written as
      ğ˜‚ = uâ‚ğ—²â‚ + uâ‚‚ğ—²â‚‚ + â€¦ uâ‚–ğ—²â‚–
    Thus Râ¿ = span{ğ—²â‚, ğ—²â‚‚, â€¦, ğ—²â‚™} is a subspace of Râ¿.

}}}
Remark 3.3.5 {{{

 1. The following are all the subspaces of RÂ²:
    a. {ğŸ¬} (given by span{ğŸ¬}),
    b. lines through the origin (given by span{ğ˜‚} for non-zero ğ˜‚ âˆˆ RÂ²),
    c. RÂ² (given by span{[1; 0], [0; 1]}).

 2. The following are all the subspaces of RÂ³:
    a. {ğŸ¬} (given by span{ğŸ¬}),
    b. lines through the origin (given by span{ğ˜‚} for non-zero ğ˜‚ âˆˆ RÂ³),
    c. planes containing the origin (given by span{ğ˜‚, ğ˜ƒ} for non-zero ğ˜‚, ğ˜ƒ âˆˆ RÂ³
       which are not parallel to each other),
    d. RÂ³ (given by span{[1; 0; 0], [0; 1; 0], [0; 0; 1]}).

}}}
Theorem 3.3.6 {{{

The solution set of a homogeneous linear system in n variables is a subspace of
Râ¿. (This subspace is called the solution space of the system)

}}}
Remark 3.3.8 (subspace condition) {{{

Let V be a non-empty subset of Râ¿. Then V is a subspace of Râ¿ if and only if:

âˆ€ğ˜‚, ğ˜ƒ âˆˆ V and âˆ€c, d âˆˆ R, cğ˜‚ + dğ˜ƒ âˆˆ V

}}}
Proofs {{{
Proof 3.3.6
If the homogeneous system has only the trivial solution, then the solution set
is {ğŸ¬} which is the zero space.

Suppose the homogeneous system has infinitely many solutions. Let
xâ‚, xâ‚‚, â€¦, xâ‚™ be the variables of the system. By solving the system, say,
using Gauss-Jordan elimintation, a general solution can be expressed in the form

xâ‚ = râ‚â‚tâ‚ + râ‚â‚‚tâ‚‚ + â€¦ + râ‚â‚–tâ‚–
xâ‚‚ = râ‚‚â‚tâ‚ + râ‚‚â‚‚tâ‚‚ + â€¦ + râ‚‚â‚–tâ‚–
                     â‹®
xâ‚™ = râ‚™â‚tâ‚ + râ‚™â‚‚tâ‚‚ + â€¦ + râ‚™â‚–tâ‚–

for some arbitrary parameters tâ‚, tâ‚‚, â€¦, tâ‚–, where râ‚â‚, râ‚â‚‚, â€¦, râ‚™â‚– âˆˆ R.
We can rewrite this general solution as

[xâ‚; = tâ‚ [râ‚â‚; + tâ‚‚ [râ‚â‚‚; + â€¦ + tâ‚– [râ‚â‚–;
 xâ‚‚;       râ‚‚â‚;       râ‚‚â‚‚;           râ‚‚â‚–;                         
 â‹® ;       â‹® ;        â‹® ;            â‹® ;
 xâ‚™]       râ‚™â‚]       râ‚™â‚‚]           râ‚™â‚–]

The solution set is
  span{
    [râ‚â‚; râ‚‚â‚; â€¦; râ‚™â‚],
    [râ‚â‚‚; râ‚‚â‚‚; â€¦; râ‚™â‚‚],
               â‹®      ,
    [râ‚â‚–; râ‚‚â‚–; â€¦; râ‚™â‚–]
  }
}}}

# Linear independence                                            @ 108  `LA-3-4`
Definition 3.4.2 {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a set of vectors in Râ¿. Consider the equation

câ‚ğ˜‚â‚ + câ‚‚ğ˜‚â‚‚ + â€¦ + câ‚–ğ˜‚â‚– = ğŸ¬

where câ‚, câ‚‚, â€¦, câ‚– are variables. Note that

câ‚ = 0, câ‚‚ = 0, â€¦, câ‚– = 0

satisfies the equation, and hence is a solution. This is called the trivial
solution.

 1. S is called a linearly independent set and ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– are said to be
    linearly independent if the equation has only the trivial solution.
 2. Otherwise:
    - S is called a linearly dependent set
    - ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– are said to be linearly dependent.
    -
}}}
Example 3.4.3 (determining linear independence) {{{

Q:
Determine whether the vectors (1, -2, 3), (5, 6, -1), (3, 2, 1) are linearly
independent.

  The equation
    câ‚(1, -2, 3) + câ‚‚(5, 6, -1) + câ‚ƒ(3, 2, 1) = (0, 0, 0)
  gives us the linear system
    câ‚ + 5câ‚‚ + 3câ‚ƒ = 0
  -2câ‚ + 6câ‚‚ + 2câ‚ƒ = 0
   3câ‚ -  câ‚‚ +  câ‚ƒ = 0
  By Gaussian Elimination, we find that there are infinitely many solutions, i.e.
  there exist non-trivial solutions. So the vectors are linearly independent.

}}}
Theorem 3.4.4 {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a set of vectors in Râ¿ where k >= 2. Then

 1. S is linearly dependent if and only if at least one vector ğ˜‚áµ¢ in S can be
    written as a linear combination of other vectors in S.
 2. S is linearly independent if and only if no vector in S can be written as a
    linear combination of other vectors in S.

}}}
Theorem 3.4.7 {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a set of vectors in Râ¿. If k > n, then S is
linearly dependent.

}}}
Theorem 3.4.10 {{{

Let ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– be linearly independent vectors in Râ¿. If ğ˜‚â‚–â‚Šâ‚ is a
vector in Râ¿ and it is not a linear combination of ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–, then
ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–, ğ˜‚â‚–â‚Šâ‚ are linearly independent.

}}}
Proofs {{{
Proof 3.4.7 {
Form a linear system to find what linear combination of the vectors will equate
to the zero vector.
This linear system with k unknowns and n equations with k > n will have
non-trivial solutions.
}
}}}

# Bases                                                          @ 112  `LA-3-5`
Discussion 3.5.1 (vector space | subspace) {{{

From now on, we shall adopt the following conventions in using the terms "vector
space" and "subspace".

 1. A set V is called a vector space if either V = Râ¿ or V is a subspace of Râ¿
    for some positive integer n.
 2. Let W be a vector space. A set V is called a subspace of W if V is a vector
    space contained in W.

}}}
Discussion 3.5.3 {{{

Given a vector space V, we want to find a set S, as small as possible, so that
every vector in V is a linear combination of the elements in S. Such a set can
then be used to build a "coordinate system" for V.

}}}
Definition 3.5.4 (basis) {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a subset of a vector space V. Then S is called
a basis for V if

 1. S is linearly independent and 
 2. S spans V.

}}}
Remark 3.5.6 {{{

 1. A basis for a vector space V contains the smallest possible number of
    vectors that can span V.
 2. For convenience, we say that the empty set, âˆ…, is the basis for the zero
    space.
 3. Except the zero space, any vector space has infinitely many different bases.

}}}
Theorem 3.5.7 (uniqueness of basis representation) {{{

If S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} is a basis for a vector space V,
then every vector ğ˜ƒ âˆˆ V can be expressed in the form

ğ˜ƒ = câ‚ğ˜‚â‚ + câ‚‚ğ˜‚â‚‚ + â€¦ + câ‚–ğ˜‚â‚–

in exactly one way,
where câ‚, câ‚‚, â€¦, câ‚– âˆˆ R.
{{{
  Since S spans V, every vector v can be expressed as a linear combination of the
  elements of S. Suppose that a vector ğ˜ƒ can be expressed in two ways
    ğ˜ƒ = câ‚ ğ˜‚â‚ + câ‚‚ ğ˜‚â‚‚ + â€¦ + câ‚– ğ˜‚â‚–
    and
    ğ˜ƒ = dâ‚ ğ˜‚â‚ + dâ‚‚ ğ˜‚â‚‚ + â€¦ + dâ‚– ğ˜‚â‚–
  Subtracting the second equation from the first, we obtain
    (câ‚ - dâ‚)ğ˜‚â‚ + (câ‚‚ - dâ‚‚)ğ˜‚â‚‚ + â€¦ + (câ‚– - dâ‚–)ğ˜‚â‚– = ğŸ¬
  Since ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚– are linearly independent, the only possible solution
  is
    câ‚ - dâ‚ = 0, câ‚‚ - dâ‚‚ = 0, â€¦, câ‚– - dâ‚– = 0
  i.e. câ‚ = dâ‚, câ‚‚ = dâ‚‚, â€¦, câ‚– = dâ‚–. So the expression is unique.
}}}

}}}
Theorem 3.5.8 {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a basis for a vector space V, and ğ˜ƒ âˆˆ V.
By Theorem 3.5.7,
ğ˜ƒ is expressed uniquely as a linear combination 

ğ˜ƒ = câ‚ğ˜‚â‚ + câ‚‚ğ˜‚â‚‚ + â€¦ + câ‚–ğ˜‚â‚–.

The coefficients câ‚, câ‚‚, â€¦, câ‚– are called
the coordinates of ğ˜ƒ relative to the basis S.

The vector (ğ˜ƒ)_S = (câ‚, câ‚‚, â€¦, câ‚–) âˆˆ Ráµ is called
the coordinate vector of ğ˜ƒ relative to the basis S.

(Here we assume the vectors of S are in a fixed order)

}}}
Example 3.5.9.3 (standard basis) {{{

Let E = {ğ—²â‚, ğ—²â‚‚, â€¦, ğ—²â‚™} where

ğ—²â‚ = [1, 0, â€¦, 0],
ğ—²â‚‚ = [0, 1, â€¦, 0],
   â‹®             ,
ğ—²â‚™ = [0, 0, â€¦, 1]

are vectors of Râ¿.

By Remark 3.3.3.2, E spans Râ¿.

Also, it is easy to show that E is linearly independent.
Thus E is a basis for Râ¿ which is called the standard basis for Râ¿.
For any ğ˜‚ = (uâ‚, uâ‚‚, â€¦, uâ‚–) âˆˆ Râ¿,

(ğ˜‚)_E = (uâ‚, uâ‚‚, â€¦, uâ‚–) = ğ˜‚

}}}
Remark 3.5.10 {{{

Let S be a basis of a vector space V.

 1. For any ğ˜‚, ğ˜ƒ âˆˆ V,

    ğ˜‚ = ğ˜ƒ <=> (ğ˜‚)_S = (ğ˜ƒ)_S

 2. For any ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£ âˆˆ V and câ‚, câ‚‚, â€¦, cáµ£ âˆˆ R,

    (câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£)_S = câ‚(ğ˜ƒâ‚)_S + câ‚‚(ğ˜ƒâ‚‚)_S + â€¦ + cáµ£(ğ˜ƒáµ£)_S.

}}}
Theorem 3.5.11 {{{

Let S be a basis for a vector space V where |S| = k.
Let ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£ be vectors in V.
Then
 1. ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£ are linearly (in)dependent vectors in V
    <=> (ğ˜ƒâ‚)_S, (ğ˜ƒâ‚‚)_S, â€¦, (ğ˜ƒáµ£)_S are linearly (in)dependent vectors in Ráµ
    {{{
      By Remark 3.5.10,

      câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£ = ğŸ¬
      => (câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£)_S = (ğŸ¬)_S
      => (câ‚ğ˜ƒâ‚)_S + (câ‚‚ğ˜ƒâ‚‚)_S + â€¦ + (cáµ£ğ˜ƒáµ£)_S = (ğŸ¬)_S

      where (ğŸ¬)_S is the zero vector in Ráµ.

      The equation câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£ = ğŸ¬ has a non-trivial solution
      if and only if the equation

      (câ‚ğ˜ƒâ‚)_S + (câ‚‚ğ˜ƒâ‚‚)_S + â€¦ + (cáµ£ğ˜ƒáµ£)_S = (ğŸ¬)_S 

      has a non-trivial solution.
    }}}
 2. span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£} = V
    <=> span{(ğ˜ƒâ‚)_S, (ğ˜ƒâ‚‚)_S, â€¦, (ğ˜ƒáµ£)_S} = Ráµ
    {{{
      Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–}
        (==>) {
          Take any vector (aâ‚, aâ‚‚, â€¦, aâ‚–) âˆˆ Ráµ.
          Let ğ˜„ = aâ‚ ğ˜‚â‚ + aâ‚‚ ğ˜‚â‚‚ + â€¦ + aâ‚– ğ˜‚â‚– âˆˆ V.
          Suppose span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£} = V.
          Then there exists real numbers câ‚, câ‚‚, â€¦, cáµ£ such that
            ğ˜„ = câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£
          By Remark 3.5.10,
            câ‚(ğ˜ƒâ‚)_S + câ‚‚(ğ˜ƒâ‚‚)_S + â€¦ + cáµ£(ğ˜ƒáµ£)_S
            = (câ‚ ğ˜ƒâ‚ + câ‚‚ ğ˜ƒâ‚‚ + â€¦ + cáµ£ ğ˜ƒáµ£)_S
            = (ğ˜„)_S
            = (aâ‚, aâ‚‚, â€¦, aâ‚–)
          This shows that every vector in Ráµ is a linear combination of
          (ğ˜ƒâ‚)_S, (ğ˜ƒâ‚‚)_S, â€¦, (ğ˜ƒáµ£)_S.
          => span{(ğ˜ƒâ‚)_S, (ğ˜ƒâ‚‚)_S, â€¦, (ğ˜ƒáµ£)_S} = Ráµ
        }
        (<==) {
          Suppose span{(ğ˜ƒâ‚)_S, (ğ˜ƒâ‚‚)_S, â€¦, (ğ˜ƒáµ£)_S} = Ráµ.
          Take any vector ğ˜„ âˆˆ V.
          Since (ğ˜„)_S âˆˆ Ráµ,
          There exists real numbers câ‚, câ‚‚, â€¦, cáµ£ such that
            (ğ˜„)_S
            = câ‚(ğ˜ƒâ‚)_S + câ‚‚(ğ˜ƒâ‚‚)_S + â€¦ + cáµ£(ğ˜ƒáµ£)_S
            = (câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£)_S
          Hence
            ğ˜„ = câ‚ğ˜ƒâ‚ + câ‚‚ğ˜ƒâ‚‚ + â€¦ + cáµ£ğ˜ƒáµ£
          This shows that every vector in V is a linear combination of
          ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£.
          => span{ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒáµ£} = V
        }
    }}}

}}}

# Dimensions                                                     @ 117  `LA-3-6`
Theorem 3.6.1 (vector space) {{{

Let V be a vector space which has a basis with k vectors. Then

 1. any subset of V with more than k vectors is always linearly dependent.
 2. any subset of V with less than k vectors cannot span V.

}}}
Remark 3.6.2 {{{

By Theorem 3.6.1, all bases for a vector space have the same number of vectors.
This number gives us a way to measure the "size" of a vector space.

}}}
Definition 3.6.3 (dimension) {{{

The dimension of a vector space V = number of vectors in a basis for V.
  = dim(V)

In addition, we define the dimension of the zero space to be zero.

}}}
Theorem 3.6.7 (basis check) {{{

Let V be a vector space, dim(V) = k, S âŠ† V.
The following are equivalent:

 1. S is a basis for V.
 2. S is linearly independent and |S| = k.
 3. S spans V (V âŠ† span(S)) and |S| = k.

best table of all time {{{
Equivalent ways to check if S is a basis of V {
 1. { (definition)
   V = span(S),
   S is linearly independent
 }
 2. {
   |S| = dim(V), 
   S âŠ† V
   S is linearly independent
 }
 3. {
   |S| = dim(V),
   V âŠ† span(S)
   => (inherently true) S âŠ† V
 }
}
}}}

}}}
Theorem 3.6.9 {{{

Let U be a subspace of V.
  Then dim(U) <= dim(V).
And if U â‰  V,
  then dim(U) < dim(V).
  (Contrapositive: if dim(U) >= dim(V), then U = V)

}}}
Theorem 3.6.11 (invertible matrices) {{{

This theorem continues from 2.4.7 and forms part of the main theorem 6.1.8. Let
ğ—” be an n Ã— n matrix. The following statements are equivalent.

 1. ğ—” is invertible.
 2. The linear system ğ—”ğ˜… = 0 has only the trivial solution.
 3. The reduced row-echelon form of ğ—” is an identity matrix.
 4. ğ—” can be expressed as a product of elementary matrices.
 5. det(ğ—”) â‰  0.
 6. The rows of ğ—” form a basis for Râ¿.
 7. The columns of ğ—” form a basis for Râ¿.

By Theorem 2.4.7, statements 1 to 4 are equivalent.
By Theorem 2.5.19, we have 1 <=> 5

The rows of ğ—” are columns of ğ—”áµ€. By 2.3.9, ğ—” is invertible <=> ğ—”áµ€ is invertible,
hence we only need to show either 1 <=> 6 or 1 <=> 7.

Let ğ—” = (ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™) where ğ—®áµ¢ is the iáµ—Ê° column of ğ—”.

{ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™} is a basis for Râ¿
<=> span{ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™} = Râ¿           (3.6.7)
<=> a row-echelon form of ğ—” has no zero row   (3.2.5.1)
<=> ğ—” is invertible                           (2.4.10)

}}}

# Transition matrices                                            @ 122  `LA-3-7`
Notation 3.7.1 (relative coordinate vector) {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a basis for a vector space V and
let ğ˜ƒ be a vector in V.
Recall that if ğ˜ƒ = câ‚ ğ˜‚â‚ + câ‚‚ ğ˜‚â‚‚ + â€¦ + câ‚– ğ˜‚â‚–, then

(ğ˜ƒ)_S = (câ‚, câ‚‚, â€¦, câ‚–)

is called the coordinate vector of ğ˜ƒ relative to S. Sometimes, it is more
convenient to write the coordinate vector in the form of a column vector.

Thus we define

[ğ˜ƒ]_S = [câ‚;
         câ‚‚;
          â‹® ;
         câ‚–]

and also call it the coordinate vector of ğ˜ƒ relative to S.

}}}
Definition 3.7.3 (transition matrix) {{{

Let S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–}
and T be two bases for a vector space V.
The square matrix ğ—£ = {[ğ˜‚â‚]_T [ğ˜‚â‚‚]_T â€¦ [ğ˜‚â‚–]_T} is called the transition
matrix from S to T.

So if ğ˜„ âˆˆ V, then

[ğ˜„]_T = ğ—£ [ğ˜„]_S

ğ—£ has to be a square matrix.

Side Note {
  V âŠ‚ Râ¿, V = span(S) = span(T)
  S = {uâ‚, uâ‚‚, ..., uâ‚˜} and T = {vâ‚, vâ‚‚, ..., vâ‚˜}
  C = (uâ‚ uâ‚‚ ... uâ‚˜)    and D = (vâ‚ vâ‚‚ ... vâ‚˜)
   
  Cáµ€D = â¡ uâ‚Â·vâ‚ uâ‚Â·vâ‚‚ â€¦ uâ‚Â·vâ‚˜ â¤
        â¢ uâ‚‚Â·vâ‚ uâ‚‚Â·vâ‚‚ â€¦ uâ‚‚Â·vâ‚˜ â¢
        â¢   â‹®     â‹®   â‹±   â‹®   â¢
        â£ uâ‚˜Â·vâ‚ uâ‚˜Â·vâ‚‚ â€¦ uâ‚˜Â·vâ‚˜ â¦ is the transition matrix from T to S
}
}}}
Example 3.7.4 (finding transition matrix) {{{

S = {ğ˜‚â‚, ğ˜‚â‚‚, ğ˜‚â‚ƒ},
  ğ˜‚â‚ = (1, 0, -1),
  ğ˜‚â‚‚ = (0, -1, 0),
  ğ˜‚â‚ƒ = (1, 0, 2).
T = {ğ˜ƒâ‚, ğ˜ƒâ‚‚, ğ˜ƒâ‚ƒ},
  ğ˜ƒâ‚ = (1, 1, 1),
  ğ˜ƒâ‚‚ = (1, 1, 0),
  ğ˜ƒâ‚ƒ = (-1, 0, 0).

(a) find the transition matrix from S to T.
First, we need to find aâ‚â‚, aâ‚‚â‚, â€¦, aâ‚ƒâ‚ƒ such that
aâ‚â‚ ğ˜ƒâ‚ + aâ‚‚â‚ ğ˜ƒâ‚‚ + aâ‚ƒâ‚ ğ˜ƒâ‚ƒ = ğ˜‚â‚
aâ‚â‚‚ ğ˜ƒâ‚ + aâ‚‚â‚‚ ğ˜ƒâ‚‚ + aâ‚ƒâ‚‚ ğ˜ƒâ‚ƒ = ğ˜‚â‚‚
aâ‚â‚ƒ ğ˜ƒâ‚ + aâ‚‚â‚ƒ ğ˜ƒâ‚‚ + aâ‚ƒâ‚ƒ ğ˜ƒâ‚ƒ = ğ˜‚â‚ƒ

â¡ 1  1 -1 |  1 |  0 |  1 â¤  rref  â¡ 1  0  0 | -1 |  0 |  2 â¤
â¢ 1  1  0 |  0 | -1 |  0 â¢   -->  â¢ 0  1  0 |  1 | -1 | -2 â¢
â£ 1  0  0 | -1 |  0 |  2 â¦        â£ 0  0  1 | -1 | -1 | -1 â¦
                                              `â†‘ ğ˜‚â‚ in terms of elements in T`
we have
-ğ˜ƒâ‚ +  ğ˜ƒâ‚‚ - ğ˜ƒâ‚ƒ = ğ˜‚â‚ `â† corresponds to that`
    -  ğ˜ƒâ‚‚ - ğ˜ƒâ‚ƒ = ğ˜‚â‚‚
2ğ˜ƒâ‚ - 2ğ˜ƒâ‚‚ - ğ˜ƒâ‚ƒ = ğ˜‚â‚ƒ

        `â†“ and finally to this`
       â¡-1  0  2â¤
So ğ—£ = â¢ 1 -1 -2â¢ is a transition matrix from S to T.
       â£-1 -1 -1â¦

`i.e. (-1, 1, -1) = (ğ˜‚â‚)_T, in line with definition 3.7.3`

}}}
}}}
# Vector spaces associated with matrices {{{                     @ 136  `LA-4`

# Row spaces and column spaces                                   @ 136  `LA-4-1`
Definition 4.1.2 (row space, column space) {{{

Let ğ—” be an m Ã— n matrix

The row space of ğ—” = span{m row vectors of ğ—”} âŠ† R^m

The column space of ğ—” = span{n column vectors of ğ—”} âŠ† Râ¿

}}}
Remark 4.1.3 {{{

row space of ğ—” = column space of ğ—”áµ€
column space of ğ—” = row space of ğ—”áµ€

}}}
Theorem 4.1.7 (row equivalent matrices) {{{

Let ğ—” and ğ—• be row equivalent matrices. Then

row space of ğ—” = row space of ğ—•

i.e. elementary row operations preserve the row space of a matrix.
{{{
  To prove this,
  let ğ—” be an arbitrary matrix with rows {râ‚, râ‚‚, â€¦, râ‚˜}
  and let ğ—• be a matrix with one row operated on by an elementary row operation
  (ráµ¢ -> kráµ¢, for example, for some i âˆˆ [1, m]).
  Show that the row space of ğ—” and ğ—• are subsets of each other.
}}}

}}}
Remark 4.1.9 {{{

Let ğ—” be a matrix and R a row-echelon form of ğ—”. Then the set of non-zero rows
in R is a basis for the row space of ğ—”.

}}}
Theorem 4.1.11 (linear relations) {{{

Let ğ—” and ğ—• be row equivalent matrices. Then

 1. A given set of columns of ğ—” is linearly independent if and only if the set of
    corresponding columns of ğ—• is linearly independent.

 2. A given set of columns for ğ—” forms a basis for the column space of ğ—” if and
    only if the set of corresponding columns of ğ—• forms a basis for the column
    space of ğ—•.

}}}
Remark 4.1.13 {{{

Extending theorem 4.1.11,

Let ğ—” be a matrix and let R be a row-echelon form of ğ—”.

A basis for the column space of ğ—” can be obtained by taking the columns of ğ—”
that correspond to the pivot columns in R.

}}}
Theorem 4.1.16 {{{

Let ğ—” be an m Ã— n matrix. Then

column space of ğ—” = { ğ—”ğ˜‚ | ğ˜‚ âˆˆ Râ¿ }

Hence âˆƒğ˜… where the system of linear equations ğ—”ğ˜… = ğ—¯ is consistent if and only
if ğ—¯ lies in the column space of ğ—”.
{{{
  Write ğ—” = (câ‚ câ‚‚ â€¦ câ‚™) where câ±¼ is the jáµ—Ê° column of ğ—”.
  For any ğ˜‚ = (uâ‚, uâ‚‚, â€¦, uâ‚™)áµ€ âˆˆ Râ¿,

  ğ—”ğ˜‚ = (câ‚ câ‚‚ â€¦ câ‚™)(uâ‚, uâ‚‚, â€¦, uâ‚™)áµ€
     = uâ‚ câ‚ + uâ‚‚ câ‚‚ + â€¦ + uâ‚™ câ‚™
     âˆˆ span{câ‚, câ‚‚, â€¦, câ‚™}
     = column space of ğ—”.
  => { ğ—”ğ˜‚ | ğ˜‚ âˆˆ Râ¿ } âŠ† column space of ğ—”.

  Conversely, suppose ğ—¯ is in the column space of ğ—”.
  i.e. ğ—¯ âˆˆ span{câ‚, câ‚‚, â€¦, câ‚™}
  then there exists uâ‚, uâ‚‚, â€¦, uâ‚™ âˆˆ R such that

  ğ—¯ = uâ‚ câ‚ + uâ‚‚ câ‚‚ + â€¦ + uâ‚™ câ‚™ = ğ—”ğ˜‚

  where ğ˜‚ = (uâ‚, uâ‚‚, â€¦, uâ‚™)áµ€.

  => column space of ğ—” âŠ† { ğ—”ğ˜‚ | ğ˜‚ âˆˆ Râ¿ }.

  so we have shown that
  column space of ğ—” = { ğ—”ğ˜‚ | ğ˜‚ âˆˆ Râ¿ }.

  Finally, a system of linear equations ğ—”ğ˜… = ğ—¯ is consistent if and only if
  there exists ğ˜‚ âˆˆ Râ¿ s.t. ğ—”ğ˜‚ = ğ—¯
  <=> ğ—¯ âˆˆ { ğ—”ğ˜‚ | ğ˜‚ âˆˆ Râ¿ } = column space of ğ—”.
}}}

}}}

# Ranks                                                          @ 145  `LA-4-2`
Theorem 4.2.1 {{{

The row space and column space of a matrix have the same dimension.
{{{
Let ğ—” be a matrix and R a row-echelon form of ğ—”.
Since the row space of ğ—” conincides with that of R (4.1.9),
we see that dim(row space of ğ—”) = number of non-zero rows in R
                                = number of pivot columns in R

On the other hand, the columns of ğ—” that correspond to the pivot columns in R
form a basis for the column space of ğ—” (4.1.13).
It follows that the dimension of the column space of ğ—” is also equal to the
number of pivot columns in R.
This completes the proof.
}}}

}}}
Definition 4.2.3 (rank) {{{

The rank of a matrix is the dimension of its row space (or column space)

Denoted by rank(ğ—”)

Note that rank(ğ—”) also equals number of non-zero rows and number of pivot
columns in a row-echelon form of ğ—”.

}}}
Remark 4.2.5 {{{

 1. For an m Ã— n matrix ğ—”, rank(ğ—”) <= min{m, n}.
    if rank(ğ—”) = min{m, n}, ğ—” is said to have full rank.
 2. A square matrix ğ—” is of full rank <=> det(ğ—”) â‰  0.
 3. rank(ğ—”) = rank(ğ—”áµ€) for any matrix ğ—”,
    because the row space of ğ—” is the column space of ğ—”áµ€
 4. rank(0) = 0, rank(ğ—œâ‚™) = n

}}}
Remark 4.2.6 (linear systems and rank) {{{

A linear system ğ—”ğ˜… = ğ—¯ is consistent if and only if ğ—” and the augmented matrix
(ğ—” | ğ—¯) have the same rank.

ğ—”ğ˜… = ğ—¯ is consistent <=> rank(ğ—”) = rank(ğ—” | ğ—¯)

}}}
Theorem 4.2.8 {{{

Let ğ—” and ğ—• be m Ã— n and n Ã— p matrices respectively. Then

rank(ğ—”ğ—•) <= min{rank(ğ—”), rank(ğ—•)}
{{{
Let ğ—” = (ğ—®â‚ ğ—®â‚‚ â€¦ ğ—®â‚™), and ğ—• = (ğ—¯â‚ ğ—¯â‚‚ â€¦ ğ—¯â‚š) where ğ—®áµ¢ and ğ—¯áµ¢ are iáµ—Ê°
columns of ğ—” and ğ—• respectively. Then

ğ—”ğ—• = (ğ—”ğ—¯â‚ ğ—”ğ—¯â‚‚ â€¦ ğ—”ğ—¯â‚š)

where ğ—”ğ—¯áµ¢ is the iáµ—Ê° column of ğ—”ğ—•.
By 4.1.16,

ğ—”ğ—¯áµ¢ âˆˆ column space of ğ—” = span{ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™}

By 3.2.10,

column space of ğ—”ğ—• = span{ğ—”ğ—¯â‚, ğ—”ğ—¯â‚‚, â€¦, ğ—”ğ—¯â‚š}
                   âŠ† span{ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™} = column space of ğ—”

So

rank(ğ—”ğ—•) = dim(column space of ğ—”ğ—•)
         <= dim(column space of ğ—”) = rank(ğ—”)

Applying this same equation to ğ—•áµ€ğ—”áµ€, we have rank(ğ—•áµ€ğ—”áµ€) <= rank(ğ—•áµ€)

Hence

rank(ğ—”ğ—•) = rank((ğ—”ğ—•)áµ€)
         = rank(ğ—•áµ€ğ—”áµ€)
         <= rank(ğ—•áµ€)
         = rank(ğ—•)

Thus we have shown that

[ rank(ğ—”ğ—•) <= rank(ğ—”) ] âˆ§ [ rank(ğ—”ğ—•) <= rank(ğ—•) ]
=> rank(ğ—”ğ—•) <= min{rank(ğ—”), rank(ğ—•)}
}}}

}}}

# Nullspaces and nullities                                       @ 147  `LA-4-3`
Definition 4.3.1 (nullspace) {{{

Let ğ—” be an m Ã— n matrix. The solution space of the homogeneous system of linear
equations ğ—”ğ˜… = 0 is known as the nullspace of ğ—”.

The dimension of the nullspace of a matrix ğ—” is called the nullity of ğ—”

Theorem 4.3.4 (dimension theorem)

Let ğ—” be a matrix with n columns. Then

rank(ğ—”) + nullity(ğ—”) = n.
{{{
  Let R be a row-echelon form of ğ—”. The columns of R can be classified into two
  types: pivot columns and non-pivot columns. Since the rank of ğ—” is the number
  of pivot columns in R and the nullity of ğ—” is the number of non-pivot columns
  in R, the theorem follows.
}}}

}}}
Theorem 4.3.6 {{{

Suppose the system of linear equations ğ—”ğ˜… = ğ—¯ has a solution ğ˜ƒ. Then the
solution set of the system is given by

M = { ğ˜‚ + ğ˜ƒ | ğ˜‚ âˆˆ nullspace of ğ—” }

}}}
Remark 4.3.7 {{{

By 4.3.6, a consistent linear system ğ—”ğ˜… = ğ—¯ has only one solution if and only if
the nullspace of ğ—” = {0}, where 0 is the zero vector.

}}}
}}}
# Orthogonality {{{                                              @ 156  `LA-5`

# The dot product                                                @ 156  `LA-5-1`
Definition 5.1.2 {{{

Let ğ˜‚ = (uâ‚, uâ‚‚, â€¦, uâ‚™), ğ˜ƒ = (vâ‚, vâ‚‚, â€¦, vâ‚™) be two vectors in Râ¿.

 1. The dot product (or inner product) of ğ˜‚ and ğ˜ƒ is defined to be the value

    ğ˜‚ Â· ğ˜ƒ = uâ‚ vâ‚ + uâ‚‚ vâ‚‚ + â€¦ uâ‚™ vâ‚™

 2. The norm (or length) of ğ˜‚ is defined to be

    ||ğ˜‚|| = âˆš(ğ˜‚ Â· ğ˜‚) = âˆš(uâ‚Â² + uâ‚‚Â² + â€¦ + uâ‚™Â²)

    In particular, vectors of norm 1 are called unit vectors.

 3. The distance between ğ˜‚ and ğ˜ƒ is
    
    d(ğ˜‚, ğ˜ƒ) = ||ğ˜‚ - ğ˜ƒ|| = âˆš[(uâ‚ - vâ‚)Â² + (uâ‚‚ - vâ‚‚)Â² + â€¦ + (uâ‚™ - vâ‚™)Â²]

 4. The angle ğœ½ between ğ˜‚ and ğ˜ƒ is

    cos^{-1}( ğ˜‚ Â· ğ˜ƒ / ||ğ˜‚|| ||ğ˜ƒ|| )

    (note that -1 <= ğ˜‚ Â· ğ˜ƒ / ||ğ˜‚|| ||ğ˜ƒ|| <= 1 so this angle is well-defined.)

}}}
Theorem 5.1.5 {{{

Let ğ˜‚, ğ˜ƒ, ğ˜„ be vectors in Râ¿ and c a scalar. Then

 1. ğ˜‚ Â· ğ˜ƒ = ğ˜ƒ Â· u
 2. (ğ˜‚ + ğ˜ƒ) Â· ğ˜„ = ğ˜‚ Â· ğ˜„ + ğ˜ƒ Â· ğ˜„,
    ğ˜„ Â· (ğ˜‚ + ğ˜ƒ) = ğ˜„ Â· ğ˜‚ + ğ˜„ Â· ğ˜ƒ
 3. (cu) Â· ğ˜ƒ = ğ˜‚ Â· (cğ˜ƒ) = c(ğ˜‚ Â· ğ˜ƒ)
 4. ||cu|| = |c| ||ğ˜‚||
 5. ğ˜‚ Â· ğ˜‚ >= 0 and ğ˜‚ Â· ğ˜‚ = 0 <=> ğ˜‚ = 0

}}}

# Orthogonal and orthonormal bases                               @ 159  `LA-5-2`
Definition 5.2.1 (orthogonal) {{{

 1. two vectors ğ˜‚ and ğ˜ƒ in Râ¿ are called orthogonal if ğ˜‚ Â· ğ˜ƒ = 0
 2. a set S of vectors in Râ¿ is called orthogonal if every pair of distinct
    vectors in S are orthogonal.
 3. a set S of vectors in Râ¿ is called orthonormal if S is orthogonal and every
    vector in S is a unit vector.

}}}
Remark 5.2.2 {{{

Given two non-zero vectors ğ˜‚ and v in Râ¿, if they are orthogonal, then the
angle between them is equal to

arccos((ğ˜‚ Â· ğ˜ƒ) / (||ğ˜‚|| ||ğ˜ƒ||)) = arccos(0) = Ï€/2

Thus the concept of "orthogonal" in Râ¿ is the same as the concept of
"perpendicular" in RÂ² and RÂ³.

}}}
Theorem 5.2.4 {{{

Let S be an orthogonal set of non-zero vectors in a vector space. Then S is
linearly independent.

}}}
Definition 5.2.5 {{{

 1. A basis S for a vector space is called an orthogonal basis if S is
    orthogonal.
 2. A basis S for a vector space is called an orthonormal basis if S is
    orthonormal.

}}}
Remark 5.2.6 {{{

By Theorem 5.2.4 and Theorem 3.6.7, to determine whether a set S of non-zero
vectors in a vector space of dimension k is an orthogonal (respectively,
orthonormal) basis, we only need to check

 1. that S is orthogonal (respectively, orthonormal), and 
 2. |S| = k

}}}
Theorem 5.2.8 {{{

 1. If S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} is an orthonormal basis for a vector space V,
    then for any vector ğ˜„ in V,

    ğ˜„ = (ğ˜„Â·ğ˜‚â‚)/(ğ˜‚â‚Â·ğ˜‚â‚) ğ˜‚â‚
       + (ğ˜„Â·ğ˜‚â‚‚)/(ğ˜‚â‚‚Â·ğ˜‚â‚‚) ğ˜‚â‚‚
                â‹®
       + (ğ˜„Â·ğ˜‚â‚–)/(ğ˜‚â‚–Â·ğ˜‚â‚–) ğ˜‚â‚–

    i.e. (ğ˜„)_S = (
           (ğ˜„Â·ğ˜‚â‚)/(ğ˜‚â‚Â·ğ˜‚â‚),
           (ğ˜„Â·ğ˜‚â‚‚)/(ğ˜‚â‚‚Â·ğ˜‚â‚‚),
                  â‹®         ,
           (ğ˜„Â·ğ˜‚â‚–)/(ğ˜‚â‚–Â·ğ˜‚â‚–)
         )

 2. If T = {ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚–} is an orthonormal basis for a vector space V,
    then for any vector ğ˜„ in V,

    ğ˜„ = (ğ˜„Â·ğ˜ƒâ‚)ğ˜ƒâ‚ + (ğ˜„Â·ğ˜ƒâ‚‚)ğ˜ƒâ‚‚ + â€¦ + (ğ˜„Â·ğ˜ƒâ‚–)ğ˜ƒâ‚–

    i.e. (ğ˜„)_T = (
           ğ˜„Â·ğ˜ƒâ‚,
           ğ˜„Â·ğ˜ƒâ‚‚,
             â‹®  ,
           ğ˜„Â·ğ˜ƒâ‚–
         )

     }}}
Definition 5.2.10 {{{

Let V be a subspace of Râ¿. A vector ğ˜‚ âˆˆ Râ¿ is said to be orthogonal (or
perpendicular) to V if ğ˜‚ is orthogonal to all vectors in V.

}}}
Remark 5.2.12 {{{

In general, if V = span{ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} is a subspace of Râ¿, then a vector
ğ˜ƒ âˆˆ Râ¿ is orthogonal to V if and only if ğ˜ƒÂ·ğ˜‚áµ¢ = 0 for all i âˆˆ {1, 2, â€¦, k}

}}}
Definition 5.2.13 {{{

Let V be a subspace of Râ¿. Every vector ğ˜‚ âˆˆ Râ¿ can be written uniquely as

ğ˜‚ = n + p

such that n is the vector orthogonal to V and p is a vector in V. The vector p
is called the (orthogonal) projection of ğ˜‚ onto V.

}}}
Theorem 5.2.15 {{{

Let V be a subspace of Râ¿ and ğ˜„ a vector in Râ¿.

 1. If {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} is an orthonormal basis for a vector space V, then

    ğ˜„â‚š = (ğ˜„Â·ğ˜‚â‚)/(ğ˜‚â‚Â·ğ˜‚â‚) ğ˜‚â‚
       + (ğ˜„Â·ğ˜‚â‚‚)/(ğ˜‚â‚‚Â·ğ˜‚â‚‚) ğ˜‚â‚‚
                â‹®
       + (ğ˜„Â·ğ˜‚â‚–)/(ğ˜‚â‚–Â·ğ˜‚â‚–) ğ˜‚â‚–

    is the projection of ğ˜„ onto V.

 2. If {ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚–} is an orthonormal basis for a vector space V, then

    ğ˜„â‚š = (ğ˜„Â·ğ˜ƒâ‚)ğ˜ƒâ‚ + (ğ˜„Â·ğ˜ƒâ‚‚)ğ˜ƒâ‚‚ + â€¦ + (ğ˜„Â·ğ˜ƒâ‚–)ğ˜ƒâ‚–

    is the projection of ğ˜„ onto V.

}}}
Remark 5.2.17 {{{

5.2.8 can be regarded as a particular case of 5.2.15 when ğ˜„ is contained in V,
i.e. when ğ˜„ = p and n = 0 in Definition 5.2.13.

}}}
Theorem 5.2.19 (Gram-Schmidt process) {{{

Let {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚–} be a basis for a vector space V. Let

ğ˜ƒâ‚ = ğ˜‚â‚

ğ˜ƒâ‚‚ = ğ˜‚â‚‚ - (ğ˜‚â‚‚Â·ğ˜ƒâ‚)/(ğ˜ƒâ‚Â·ğ˜ƒâ‚) ğ˜ƒâ‚

ğ˜ƒâ‚ƒ = ğ˜‚â‚ƒ - (ğ˜‚â‚ƒÂ·ğ˜ƒâ‚)/(ğ˜ƒâ‚Â·ğ˜ƒâ‚) ğ˜ƒâ‚ - (ğ˜‚â‚ƒÂ·ğ˜ƒâ‚‚)/(ğ˜ƒâ‚‚Â·ğ˜ƒâ‚‚) ğ˜ƒâ‚‚

â€¦

ğ˜ƒâ‚– = ğ˜‚â‚–
     - (ğ˜‚â‚–Â·ğ˜ƒâ‚)/(ğ˜ƒâ‚Â·ğ˜ƒâ‚) ğ˜ƒâ‚
     - (ğ˜‚â‚–Â·ğ˜ƒâ‚‚)/(ğ˜ƒâ‚‚Â·ğ˜ƒâ‚‚) ğ˜ƒâ‚‚
                â‹®
     - (ğ˜‚â‚–Â·ğ˜ƒâ‚–-1)/(ğ˜ƒâ‚–-1Â·ğ˜ƒâ‚–-1) ğ˜ƒâ‚–-1

Then {ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚–} is an orthogonal basis for V. Furthermore, let

ğ˜„â‚ = 1/||ğ˜ƒâ‚|| ğ˜ƒâ‚
ğ˜„â‚‚ = 1/||ğ˜ƒâ‚‚|| ğ˜ƒâ‚‚
    â‹®
ğ˜„â‚– = 1/||ğ˜ƒâ‚–|| ğ˜ƒâ‚–

Then {ğ˜„â‚, ğ˜„â‚‚, â€¦, ğ˜„â‚–} is an orthonormal basis for V.

}}}

# Best approximations                                            @ 166  `LA-5-3`
Theorem 5.3.2 {{{

Let V be a subspace in Râ¿. If ğ˜‚ is a vector in Râ¿ and p is the projection of u
onto V, then

d(ğ˜‚, ğ—½) <= d(ğ˜‚, ğ˜ƒ) for all ğ˜ƒ âˆˆ V.

i.e. ğ—½ is the best approximation of ğ˜‚ in V.

}}}
Definition 5.3.6 {{{

Let ğ—”ğ˜… = ğ—¯ be a linear system where ğ—” is an m Ã— n matrix. A vector ğ˜‚ âˆˆ Râ¿ is
called a least squares solution to the linear system if ||ğ—¯ - ğ—”ğ˜‚|| <= ||ğ—¯ - ğ—”ğ˜ƒ||
for all ğ˜ƒ âˆˆ Râ¿.

}}}
Theorem 5.3.8 {{{

Let ğ—”ğ˜… = ğ—¯ be a linear system, where ğ—” is an m Ã— n matrix, and let ğ—½ be the
projection of ğ—¯ onto the column space of ğ—”. Then

||ğ—¯ - ğ—½|| <= ||ğ—¯ - ğ—”ğ˜ƒ|| for all ğ˜ƒ âˆˆ Râ¿.

i.e. ğ˜‚ is a least squares solution to ğ—”ğ˜… = ğ—¯ if and only if ğ—”ğ˜‚ = ğ—½.

}}}
Theorem 5.3.10 (finding least squares) {{{

Let ğ—”ğ˜… = ğ—¯ be a linear system. Then ğ˜‚ is a least squares solution to ğ—”ğ˜… = ğ—¯ if
and only if ğ˜‚ is a solution to ğ—”áµ€ğ—”ğ˜… = ğ—”áµ€ğ—¯
{{{

Let ğ—” = (ğ—®â‚ ğ—®â‚‚ â€¦ ğ—®â‚™), where aáµ¢ is the iáµ—Ê° column of ğ—”, and let V be the
column space of ğ—”.
i.e. V = span{ğ—®â‚ ğ—®â‚‚ â€¦ ğ—®â‚™} = { ğ—”ğ˜ƒ | ğ˜ƒ âˆˆ Râ¿ }. Then

ğ˜‚ is a least squares solution to ğ—”ğ˜… = ğ—¯
<=> ğ—”ğ˜‚ is the projection of ğ—¯ onto V
<=> ğ—¯ - ğ—”ğ˜‚ is orthogonal to V
<=> ğ—¯ - ğ—”ğ˜‚ is orthogonal to ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™
<=> ğ—®â‚Â·(ğ—¯ - ğ—”ğ˜‚) = 0, ğ—®â‚‚Â·(ğ—¯ - ğ—”ğ˜‚) = 0, â€¦, ğ—®â‚™Â·(ğ—¯ - ğ—”ğ˜‚) = 0.
<=> ğ—”áµ€(ğ—¯ - ğ—”ğ˜‚) = 0
<=> ğ—”áµ€ğ—”ğ˜‚ = ğ—”áµ€ğ—¯

}}}

}}}

# Orthogonal matrices                                            @ 171  `LA-5-4`
Definition 5.4.3 (orthogonal matrix) {{{

A square matrix ğ—” is called orthogonal if ğ—”â»Â¹ = ğ—”áµ€
A non-square matrix cannot be an orthogonal matrix.

}}}
Remark 5.4.4 {{{

By Theorem 2.4.12, a square matrix ğ—” is orthogonal if and only if
ğ—”ğ—”áµ€ = ğ—œ (or ğ—”áµ€ğ—” = ğ—œ)

}}}
Theorem 5.4.6 {{{

Let ğ—” be a square matrix of order n. The following statements are equivalent:

 1. ğ—” is an orthogonal matrix
 2. The rows of ğ—” form an orthonormal basis for Râ¿
 3. The columns of ğ—” form an orthonormal basis for Râ¿
{{{

1 <=> 2:

Let ğ—” = (ğ—®â‚; ğ—®â‚‚; â€¦ ; ğ—®â‚™) where aáµ¢ is the iáµ—Ê° row of ğ—”.

By Remark 5.2.6, it suffices to show that
(ğ—” is orthogonal) <=> ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™ are orthonormal.

Observe that ğ—”ğ—”áµ€ = [ğ—®â‚Â·ğ—®â‚ ğ—®â‚Â·ğ—®â‚‚ â€¦ ğ—®â‚Â·ğ—®â‚™;
                    ğ—®â‚‚Â·ğ—®â‚ ğ—®â‚‚Â·ğ—®â‚‚ â€¦ ğ—®â‚‚Â·ğ—®â‚™;
                                    â‹®
                    ğ—®â‚™Â·ğ—®â‚ ğ—®â‚™Â·ğ—®â‚™ â€¦ ğ—®â‚™Â·ğ—®â‚™]

By Remark 5.4.4,
ğ—” is orthogonal <=> ğ—”ğ—”áµ€ = ğ—œ
                <=> for all i, j, ğ—®áµ¢Â·ğ—®â±¼ = 1, if i = j
                                          = 0, if i â‰  j
                <=> ğ—®â‚, ğ—®â‚‚, â€¦, ğ—®â‚™ are orthonormal.

The proof of 1 <=> 3 is similar except we compute ğ—”áµ€ğ—” instead.

}}}

}}}
Theorem 5.4.7 {{{

Let S and T be two orthonormal bases for a vector space and let ğ—£ be the
transition matrix from S to T. Then ğ—£ is orthonormal and ğ—£áµ€ is the transition
matrix from T to S.

}}}
}}}
# Diagonalization {{{                                            @ 184  `LA-6`

# Eigenvalues and eigenvectors                                   @ 184  `LA-6-1`
Definition 6.1.3 (eigenvector, eigenvalue) {{{

Let ğ—” be a square matrix of order n. A non-zero column vector ğ˜‚ in Râ¿ is called
an eigenvector of ğ—” if

ğ—”ğ˜‚ = ğºğ˜‚

for some scalar ğº. The scalar ğº is called an eigenvalue of ğ—” and ğ˜‚ is said to be
an eigenvector of ğ—” associated with the eigenvalue ğº.

}}}
Remark 6.1.5 {{{

Let ğ—” be a square matrix of order n. Then

ğº is an eigenvalue of ğ—”.
<=> ğ—”ğ˜‚ = ğºğ˜‚ for some non-zero column vector ğ˜‚ âˆˆ Râ¿
<=> ğºğ˜‚ - ğ—”ğ˜‚ = 0
<=> (ğºğ—œ - ğ—”)ğ˜‚ = 0
<=> the linear system (ğºğ—œ - ğ—”)ğ˜… = 0 has non-trivial solutions
<=> det(ğºğ—œ - ğ—”) = 0

If expanded, det(ğºğ—œ - ğ—”) is a polynomial in ğº of degree n.

}}}
Definition 6.1.6 {{{

Let ğ—” be a square matrix of order n. THe equation

det(ğºğ—œ - ğ—”) = 0

is called the characteristic equation of ğ—” and the polynomial

det(ğºğ—œ - ğ—”)

is called the characteristic polynomial of ğ—”.

}}}
Theorem 6.1.8 (main theorem on invertible matrices, swiss army knife) {{{

Let ğ—” be an n Ã— n matrix. The following statements are equivalent.

 1. ğ—” is invertible.
 2. The linear system ğ—”ğ˜… = 0 has only the trivial solution.
 3. The reduced row-echelon form of ğ—” is an identity matrix.
 4. ğ—” can be expressed as a product of elementary matrices.
 5. det(ğ—”) â‰  0.
 6. The rows of ğ—” form a basis for Râ¿.
 7. The columns of ğ—” form a basis for Râ¿.
 8. rank(ğ—”) = n.
 9. 0 is not an eigenvalue of ğ—”.
 10. Ax = b has a unique solution x, if consistent. `added`

}}}
Theorem 6.1.9 {{{

If ğ—” is a triangular matrix (either upper or lower), the eigenvalues of ğ—” are
the diagonal entries of ğ—”.
{{{

Suppose ğ—” = (aáµ¢â±¼)â‚™Ã—n is a triangular matrix. Then ğºğ—œ - ğ—” is a triangular
matrix with diagonal entries ğº - ğ—®â‚â‚, ğº - ğ—®â‚‚â‚‚, â€¦, ğº - ğ—®â‚™â‚™. By 2.5.8,

det(ğºğ—œ - ğ—”) = (ğº - ğ—®â‚â‚)(ğº - ğ—®â‚‚â‚‚) â€¦ (ğº - ğ—®â‚™â‚™)

Hence the diagonal entries ğ—®â‚â‚, ğ—®â‚‚â‚‚, â€¦, ğ—®â‚™â‚™ of ğ—” are the eigenvalues of ğ—”.

}}}

}}}
Definition 6.1.11 {{{

Let ğ—” be a square matrix of order n and ğº an eigenvalue of ğ—”.

Then the solution space of the linear system (ğºğ—œ - ğ—”)ğ˜… = 0 is called the
eigenspace of ğ—” associated with the eigenvalue ğº and is denoted by E_ğº.

Note that if ğ˜‚ is a non-zero vector in E_ğº, then ğ˜‚ is an eigenvector of ğ—”
associated with the eigenvalue ğº.

}}}

# Diagonalization                                                @ 191  `LA-6-2`
Definition 6.2.1 (diagonalizability) {{{

A square matrix ğ—” is called diagonalizable if there exists an invertible matrix
ğ—£ such that ğ—£â»Â¹ğ—”ğ—£ is a diagonal matrix. Here the matrix ğ—£ is said to
diagonalize ğ—”.

}}}
Theorem 6.2.3 (conditions for diagonalization) {{{

Let ğ—” be a square matrix of order n. Then ğ—” is diagonalizable if and only if ğ—”
has n linearly independent eigenvectors.

}}}
Algorithm 6.2.4 {{{

Given a square matrix ğ—” of order n, we want to determine whether ğ—” is
diagonalizable. Also, if ğ—” is diagonalizable, find an invertible matrix ğ—£ such
that ğ—£â»Â¹ğ—”ğ—£ is a diagonal matrix.

 1. Find all distinct eigenvalues ğºâ‚, ğºâ‚‚, â€¦, ğºâ‚–. (By 6.1.5, eigenvalues
    can be obtained by solving the characteristic equation det(ğºğ—œ - ğ—”) = 0)
 2. For each eigenvalue ğº, find a basis S_ğº, for the eigenspace E_ğº.
 3. Let S = S_(ğºâ‚) âˆª S_(ğºâ‚‚) âˆª â€¦ âˆª S_(ğºâ‚–)
    a. if |S| < n, then ğ—” is not diagonalizable.
    b. if |S| = n, say S = {ğ˜‚â‚, ğ˜‚â‚‚, â€¦, ğ˜‚â‚™}, then ğ—£ = (ğ˜‚â‚ ğ˜‚â‚‚ â€¦ ğ˜‚â‚™) is
    an invertible matrix that diagonalizes ğ—”.

}}}
Theorem 6.2.7 {{{

Let ğ—” be a square matrix of order n. If ğ—” has n distinct eigenvalues, then ğ—” is
diagonalizable.
{{{

Suppose ğ—” has n distinct eigenvalues. In Step 2 of Algorithm 6.2.4, we can find
one eigenvector for each eigenvalue and hence we have n eigenvectors. By
6.2.5.3, these eigenvectors are linearly independent. So ğ—” is diagonalizable.

}}}

}}}

# Orthogonal diagonalization                                     @ 198  `LA-6-3`
Definition 6.3.2 (orthogonal diagonalizability) {{{

if there exists an orthogonal matrix ğ—£ such that ğ—£áµ€ğ—”ğ—£ is a diagonal matrix.
Here, ğ—£ is said to orthogonally diagonalize ğ—”.

}}}
Theorem 6.3.4 (condition for orthogonal diagonalizability) {{{

A square matrix is orthogonally diagonalizable if and only if it is symmetric.

}}}
Algorithm 6.3.5 {{{

Given a symmetric matrix ğ—” of order n, we want to find an orthogonal matrix ğ—£
such that ğ—£áµ€ğ—”ğ—£ is a diagonal matrix.

 1. Find all distinct eigenvalues ğºâ‚, ğºâ‚‚, â€¦, ğºâ‚–.
 2. For each eigenvalue ğºáµ¢,
    a. find a basis S_(ğºáµ¢) for the eigenspace E_(ğºáµ¢)
    b. use the Gram-Schmidt process (5.2.19) to transform S_(ğºáµ¢) to an
    orthonormal basis T_(ğºáµ¢)
 3. Let T = T_(ğºâ‚) âˆª T_(ğºâ‚‚) âˆª â€¦ âˆª T_(ğºâ‚–), say T = {ğ˜ƒâ‚, ğ˜ƒâ‚‚, â€¦, ğ˜ƒâ‚™}
    Then ğ—£ = (ğ˜ƒâ‚ ğ˜ƒâ‚‚ â€¦ ğ˜ƒâ‚™) is an orthogonal matrix that diagonalizes ğ—”.

}}}
Remark 6.3.6 (about Algorithm 6.3.5){{{

 1. In Step 1, the eigenvalues of a symmetric matrix are always real numbers.
 2. Since ğ—” is diagonalizable, by 6.2.5.2, we have the following result:
    Suppose the characteristic polynomial of the matrix ğ—” can be factorized as

    det(ğºğ—œ - ğ—”) = (ğº - ğºâ‚)^{râ‚} Â· (ğº - ğºâ‚‚)^{râ‚‚} Â· â€¦ Â· (ğº - ğºâ‚–)^{râ‚–}

    where ğºâ‚, ğºâ‚‚, â€¦, ğºâ‚– are distinct eigenvalues of ğ—”. Then in Step 2, for
    each eigenvalue ğºâ‚, dim(E_(ğºáµ¢)) = ráµ¢, i.e. |S_(ğºáµ¢)| = |T_(ğºáµ¢)| = ráµ¢.
 3. In Step 3, the set T is always orthogonal.
 4. Since T is always orthogonal, by 5.4.6, the square matrix ğ—£ in Step 3 is
    always orthogonal.

}}}

# Quadratic forms and conic section                              @ 202  `LA-6-4`
Definition 6.4.1 {{{

The expression

Q(xâ‚, xâ‚‚, â€¦, xâ‚™) = ğ¨_{i = 1}â¿ { ğ¨_{j = i}â¿ { qáµ¢â±¼ xáµ¢ xâ±¼ }}
                  = qâ‚â‚ (xâ‚)Â² + qâ‚â‚‚ xâ‚ xâ‚‚ + â€¦ + qâ‚â‚™ xâ‚ xâ‚™
                              + qâ‚‚â‚‚ (xâ‚‚)Â² + â€¦ + qâ‚‚n xâ‚ xâ‚™
                                          + â€¦ 
                                              + qâ‚™â‚™ (xâ‚™)Â²

where qáµ¢â±¼ are real numbers, is called a quadratic form in n variables
xâ‚, xâ‚‚, â€¦, xâ‚™.

Define an n Ã— n symmetric matrix ğ—” = (aáµ¢â±¼) such that

aáµ¢â±¼ = qáµ¢áµ¢      if i = j,
    = 1/2 qáµ¢â±¼  if i < j,
    = 1/2 qâ±¼áµ¢  if i > j

and let ğ˜… = (xâ‚, xâ‚‚, â€¦, xâ‚™)áµ€ Then

Q(xâ‚, xâ‚‚, â€¦, xâ‚™)
= (xâ‚ xâ‚‚ â€¦ xâ‚™) * [   qâ‚â‚   1/2Â·qâ‚â‚‚ â€¦ 1/2Â·qâ‚â‚™; * [xâ‚;
                   1/2Â·qâ‚â‚‚   qâ‚‚â‚‚   â€¦ 1/2Â·qâ‚‚â‚™;    xâ‚‚;
                      â‹®            â‹±    â‹®        â‹® ;
                   1/2Â·qâ‚â‚™ 1/2Â·qâ‚‚â‚™ â€¦   qâ‚™â‚™  ]    xâ‚™]
= ğ˜…áµ€ğ—”ğ˜…

Thus the quadratic form can be regarded as a mapping Q : Râ¿ -> R defined by

Q(ğ˜…) = ğ˜…áµ€ğ—”ğ˜…  for ğ˜… âˆˆ Râ¿

}}}
Definition 6.4.6 {{{

A quadratic equation in two variables x and y is an equation of the form

axÂ² + bxy + cyÂ² + dx + ey = f

where a, b, c, d, e, f are real numbers and a, b, c are not all zero. We can
rewrite the equation in the form

[x y] * [  a   1/2Â·b; * [x;  +  [d e] * [x; = f
         1/2Â·b   c  ]    y]              y]

Let ğ˜… = [x; y], ğ—” = [a 1/2Â·b; 1/2Â·b c], and ğ—¯ = [d e]. Then the equation becomes

ğ˜…áµ€ğ—”ğ˜… + ğ—¯áµ€ğ˜… = f

The term axÂ² + bxy + cyÂ² (= ğ˜…áµ€ğ—”ğ˜…) is called the quadratic form associated with
the quadratic equation.

}}}

}}}
# Linear transformations {{{                                     @ 216  `LA-7`
# Linear transformations from Râ¿ to Ráµ                           @ 216  `LA-7-1`

Definition 7.1.1

A linear transformation is a mapping T : Râ¿ -> Ráµ.

T(x) = y,
for some x âˆˆ Râ¿ and y âˆˆ Ráµ

The m Ã— n matrix ğ—” such that ğ—”ğ˜… = ğ˜†
is called the standard matrix for T.

Theorem 7.1.4

Let T : Râ¿ -> Ráµ be a linear transformation.

 1. T(ğŸ¬) = ğŸ¬
 2. If uâ‚, uâ‚‚, â€¦, uâ‚– âˆˆ Râ¿ and câ‚, câ‚‚, â€¦, câ‚– âˆˆ R, then
    T(câ‚uâ‚ + câ‚‚uâ‚‚ + â€¦ + câ‚–uâ‚–) = câ‚T(uâ‚) + câ‚‚T(uâ‚‚) + â€¦ + câ‚–T(uâ‚–)

Definition 7.1.10

Let S : Râ¿ -> Ráµ and T : Ráµ -> Ráµ be linear transformations.
The composition of T with S, T âˆ˜ S, is a mapping from Râ¿ to Ráµ

(T âˆ˜ S)(u) = T(S(u)) for u âˆˆ Râ¿

Theorem 7.1.11

If S : Râ¿ -> Ráµ and T : Ráµ -> Ráµ are linear transformations, then
T âˆ˜ S : Râ¿ -> Ráµ is again a linear transformation.

Furthermore, if A and B are the standard matrices for the linear
transformations S and T respectively, then the standard matrix for
T âˆ˜ S is given by BA.

# Ranges and kernels                                             @ 221  `LA-7-2`

Definition 7.2.1

Let T : Râ¿ -> Ráµ be a linear transformation.
The range of T, denoted by R(T), is the set of images of T:

R(T) = { T(u) | u âˆˆ Râ¿ } âŠ† Ráµ

Theorem 7.2.4

Let T : Râ¿ -> Ráµ be a linear transformation and
A be the standard matrix for T. Then

R(T) = column space of A

which is a subspace of Ráµ.

Definition 7.2.5

Let T be a linear transformation.
The dimension of R(T) is called the rank of T,
denoted by rank(T)

By Theorem 7.2.4, if A is the standard matrix of T,
then rank(T) = rank(A).

Theorem 7.2.9

Let T : Râ¿ -> Ráµ be a linear transformation and
A be the standard matrix for T. Then

Ker(T) = nullspace of A

which is a subspace of Ráµ.

Definition 7.2.10

Let T be a linear transformation.
The dimension of Ker(T) is called the nullity of T,
denoted by nullity(T).

By Theorem 7.2.9, if A is the standard matrix of T,
then nullity(T) = nullity(A).

Theorem 7.2.12 (Dimension Theorem for Linear Transformations)

If T : Râ¿ -> Ráµ is a linear transformation, then

rank(T) + nullity(T) = n

# Geometric linear transformations                               @ 225  `LA-7-3`

}}}
